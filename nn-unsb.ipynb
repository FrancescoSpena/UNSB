{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Re-Implementation of \"Unpaired Image-to-Image Translation via Neural Schrödinger Bridge\""]},{"cell_type":"markdown","metadata":{},"source":["#### This notebook presents a re-implementation of the methods proposed in the paper \"[Unpaired Image-to-Image Translation via Neural Schrödinger Bridge](https://arxiv.org/abs/2305.15086)\". The original paper introduces a novel approach to address the limitations of traditional diffusion models in unpaired image-to-image (I2I) translation tasks. Diffusion models, which are generative models simulating stochastic differential equations (SDEs), often rely on a Gaussian prior assumption, which may not be suitable for all types of data distributions.\n","\n","#### The concept of Schrödinger Bridge (SB) offers a promising solution by learning an SDE that facilitates translation between two arbitrary distributions. This paper proposes the Unpaired Neural Schrödinger Bridge (UNSB), which treats the SB problem through a series of adversarial learning challenges. This adaptation enables the use of advanced discriminators and regularization techniques, improving the model's ability to learn and translate between unpaired data sets effectively.\n","\n","#### The re-implementation in this notebook aims to explore the scalability and efficiency of UNSB, demonstrating its capability to perform various unpaired I2I translation tasks, particularly focusing on high-resolution images where previous SB models have faced challenges."]},{"cell_type":"markdown","metadata":{},"source":["## All necessary Imports & Set-Up\n","#### As a first step of our Project, we proceed to Import all the necessary Libraries we will using throughout the whole project and to Set-Up the necessary environment for the project to work."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee788347-3a9a-47ec-83e0-fe558a894fc7","_uuid":"a426403f-d3b4-466e-ac7d-b7fd98deea0c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import numpy as np \n","import os \n","import cv2\n","import torch \n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","from torchvision import transforms\n","import torch.nn.functional as F\n","import functools\n","from PIL import Image\n","import torchvision \n","from torchvision.utils import save_image\n","import torch.optim as optim\n","import math\n","from torch.nn import init\n","import torchvision.utils as vutils\n","import time\n","import torchvision.models as models\n","from scipy.linalg import sqrtm\n","from tqdm import tqdm\n","import sys \n","from sklearn.metrics.pairwise import polynomial_kernel\n","from torch.nn.functional import adaptive_avg_pool2d\n","import pathlib\n","import torchvision.transforms as TF\n","from scipy import linalg\n","import matplotlib.image as mpimg"]},{"cell_type":"markdown","metadata":{},"source":["#### In the following section of the code, we configure the processing device for our neural network operations. \n","#### Using CUDA, PyTorch can leverage the GPU’s capabilities to significantly speed up the computations necessary for neural network training. If a GPU is not available, the code defaults to using the CPU.\n","#### The model was trained on \"Kaggle\", using GPU P100."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4779f7a5-2855-46b5-a792-2fb7f80d1883","_uuid":"a6f5a1fd-2a3d-444d-b472-e5aeb8f424ef","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Set up processing device; use GPU via CUDA if available, otherwise fallback to CPU.\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{},"source":["## Loading the Dataset"]},{"cell_type":"markdown","metadata":{},"source":["#### For our image translation experiments, we have selected the horse2zebra dataset. This dataset is widely used in image-to-image translation tasks and provides a diverse set of images of horses and zebras, making it an ideal candidate for testing the capabilities of the Unpaired Neural Schrödinger Bridge (UNSB) model in performing complex translation tasks between these two distinct classes of images.\n","\n","#### Due to limited computational resources, we opted to train the model on a smaller subset of the horse2zebra dataset. \n","#### Specifically:\n","\n","#### - **Training Datasets**: each of the horse and zebra categories in the training set contains 200 images, totaling 400 images for model training. This allows sufficient variability to challenge the model's ability to learn meaningful translations without overwhelming our available resources.\n","### - **Testing Datasets**: for testing, each category—horse and zebra—includes 120 images. This selection ensures that we can evaluate the model's performance on unseen data, providing a robust measure of its generalization capabilities across different inputs.\n","\n","#### The selection of these subsets was aimed at maintaining a diverse representation of images to ensure that the model can still learn to generalize well across different types of input. By focusing on a manageable subset of images, we can efficiently test and iterate our model without the need for extensive computational power, which is often a constraint in academic or small-scale research settings. This approach enables us to gain valuable insights into the model's performance and scalability under restricted conditions, while still ensuring comprehensive exposure to the variability present in real-world scenarios."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Paths to the training datasets for both domains:\n","path_trainA = '/kaggle/input/horse2zebra-new/horse2zebra/small_trainA'  # Directory path for 'small_trainA' containing a subset of horse images.\n","path_trainB = '/kaggle/input/horse2zebra-new/horse2zebra/small_trainB' # Directory path for 'small_trainB' containing a subset of zebra images.\n","\n","# Paths to the testing datasets for both domains:\n","path_testA = '/kaggle/input/horse2zebra-new/horse2zebra/testA'  # Directory path for 'testA' containing horse images for testing.\n","path_testB = '/kaggle/input/horse2zebra-new/horse2zebra/testB'  # Directory path for 'testB' containing zebra images for testing."]},{"cell_type":"markdown","metadata":{},"source":["## Creating a Directory for Generated Images\n","#### In the following section of the code, we prepare for storing the output of our image translation model by setting up a directory where generated images will be saved. This is an essential step in organizing the outputs for evaluation and visualization purposes.\n","\n","#### The code checks if a directory named generated_images exists in the /kaggle/working directory. If it does not exist, the directory is created. This setup ensures that we have a designated place to store our model's outputs without any interruptions during runtime."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"758f086c-26af-4762-97cb-e6ea368c205c","_uuid":"95972e57-0fe9-4818-bd28-a2806b0609e8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Create output images directories\n","generated_images = '/kaggle/working/generated_images'\n","if not os.path.exists(generated_images):\n","    os.makedirs('/kaggle/working/generated_images')"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preparation"]},{"cell_type":"markdown","metadata":{},"source":["#### In this section, we define a custom dataset class, ImageDataset, which inherits from PyTorch's Dataset class. This class is specifically tailored to handle image data for neural network training. Here’s how it works:\n","\n","#### - Initialization (__init__): The constructor takes the directory path (img_dir) where the images are stored. It then traverses the directory to collect paths to all images with specified extensions (.png, .jpg, .jpeg), ensuring that only image files are included. Additionally, a transformation pipeline is defined to process the images, which includes converting them to PIL format, resizing, tensor conversion, and normalization.\n","\n","#### - Length (__len__): This method returns the total number of images in the dataset, allowing PyTorch to calculate the number of samples.\n","\n","#### - Get Item (__getitem__): This method retrieves an image by index, reads it from the disk, processes it through the specified transformations, and returns the final tensor ready for model input.\n","\n","#### This structure facilitates the handling and preprocessing of image data, ensuring that the images are in the correct format and dimensions required by the neural network."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a3543dbd-1083-427b-a4d4-730820791d83","_uuid":"2a7418bf-7050-422e-817c-1cf1e8cf9b1c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["class ImageDataset(Dataset):\n","    def __init__(self, img_dir):\n","        self.img_dir = img_dir  # Store the directory path to images\n","        self.image_paths = []  # List to hold the paths of the images\n","        \n","        # Walk through the directory to collect all image file paths\n","        for root, dirs, files in os.walk(img_dir):\n","            for file in files:\n","                # Add paths for files with appropriate image extensions\n","                if file.endswith(('.png', '.jpg', '.jpeg')):  \n","                    self.image_paths.append(os.path.join(root, file))\n","\n","        # Define transformations for image preprocessing\n","        self.transform = transforms.Compose([\n","            transforms.ToPILImage(),  # Convert numpy arrays to PIL images for further processing\n","            transforms.Resize((256, 256)),  # Resize images to uniform size for the model\n","            transforms.ToTensor(),  # Convert images to PyTorch tensors\n","            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize pixel values to [-1, 1]\n","        ])\n","\n","    def __len__(self):\n","        # Return the total number of images available in the dataset\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        # Fetch the path and load the image\n","        image_path = self.image_paths[idx]\n","        image = cv2.imread(image_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert color from BGR to RGB\n","\n","        # Apply the predefined transformations to the image\n","        if self.transform:\n","            image = self.transform(image)\n","        \n","        return image  # Return the transformed image"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset and DataLoader Configuration\n","#### In this section, we initialize our datasets and dataloaders for both the training and testing phases. The horse2zebra dataset has been split into distinct sets for horses (TrainA and TestA) and zebras (TrainB and TestB).\n","\n","#### Due to our limited computational resources, we set the batch size to 1. While this is not optimal for general training scenarios as it might lead to higher variance during the training process, it allows for deep model testing and demonstrations on machines with limited memory."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Set the batch size for training the model to 1, reflecting our resource limitations.\n","BATCH_SIZE = 1  # Smaller batch size is not ideal for performance but necessary for our resource-constrained environment."]},{"cell_type":"markdown","metadata":{},"source":["#### We use the custom ImageDataset class, previously defined, to handle the loading and preprocessing of images from designated directory paths. This class ensures that images are correctly resized, normalized, and transformed into tensors, making them suitable for input into our neural network."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Initialize datasets for training and testing using the predefined ImageDataset class.\n","train_datasetA = ImageDataset(img_dir=path_trainA)  # Dataset of horse images for training.\n","train_datasetB = ImageDataset(img_dir=path_trainB)  # Dataset of zebra images for training.\n","test_datasetA = ImageDataset(img_dir=path_testA)    # Dataset of horse images for testing.\n","test_datasetB = ImageDataset(img_dir=path_testB)    # Dataset of zebra images for testing."]},{"cell_type":"markdown","metadata":{},"source":["#### DataLoaders are now used to efficiently load data in batches, shuffle the data for better generalization, and enable multi-threading to expedite the data loading process. Given our small batch size, shuffling helps in reducing sample correlation and prevents the model from memorizing the sequence of training examples."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create DataLoaders for the datasets to manage loading and batching of data during training and testing.\n","train_dataloaderA = DataLoader(train_datasetA, batch_size=BATCH_SIZE, shuffle=True)  # DataLoader for horse images in training.\n","train_dataloaderB = DataLoader(train_datasetB, batch_size=BATCH_SIZE, shuffle=True)  # DataLoader for zebra images in training.\n","test_dataloaderA = DataLoader(test_datasetA, batch_size=BATCH_SIZE, shuffle=True)     # DataLoader for horse images in testing.\n","test_dataloaderB = DataLoader(test_datasetB, batch_size=BATCH_SIZE, shuffle=True)     # DataLoader for zebra images in testing."]},{"cell_type":"markdown","metadata":{},"source":["## Helper Functions for Model Construction"]},{"cell_type":"markdown","metadata":{},"source":["#### This section details the utility and helper functions that are integral to building, initializing, and visualizing outcomes from our model. These functions handle various tasks such as padding, normalization, weight initialization, and image visualization. Each function is designed to modularize the codebase, making it easier to maintain and understand.\n","\n","#### - **Padding Function**: *get_pad_layer* - It returns a padding layer based on the specified type (reflect, replicate, or zero). It is used to add padding to the inputs of convolutional layers, ensuring that boundary effects are handled correctly.\n","\n","#### - **Pixel Normalization**: *PixelNorm* - A normalization technique used in generator models to stabilize the training process. It normalizes the feature vectors to a unit length, which helps in controlling the scale of gradients during backpropagation.\n","\n","#### - **Timestep Embedding**: Functions and classes (*get_timestep_embedding* and *TimestepEmbedding*) related to embedding timesteps into inputs. This is particularly useful in models that need to consider the sequence of data, like time-series analysis or specific generative models where the sequence of generation is crucial.\n","\n","#### - **Weight Initialization**: *init_weights* and *init_net* - These functions initialize the weights of the network in a way that aims to improve convergence during training. Standard practices like initializing weights from a Gaussian distribution and setting biases to zero are followed.\n","\n","#### - **Normalization and Visualization**: *Normalize* - Applies a power normalization to the tensor. *denormalize* and *visualize_images* are utility functions used to convert tensor values to image format and display them. These are particularly useful during testing and when evaluating the model's performance visually."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define padding layer based on type for use in convolutional layers\n","def get_pad_layer(pad_type):\n","    if pad_type in ['reflect', 'refl']:\n","        return nn.ReflectionPad2d\n","    elif pad_type in ['replicate', 'repl']:\n","        return nn.ReplicationPad2d\n","    elif pad_type == 'zero':\n","        return nn.ZeroPad2d\n","    else:\n","        raise NotImplementedError(f'Padding type {pad_type} not recognized')\n","\n","# Module to normalize pixel values in images for stable training\n","class PixelNorm(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, input):\n","        return input * torch.rsqrt(torch.mean(input ** 2, dim=1, keepdim=True) + 1e-8)\n","        \n","# Generate embeddings for timesteps in models that incorporate time dynamics\n","def get_timestep_embedding(timesteps, embedding_dim, max_positions=10000):\n","    assert len(timesteps.shape) == 1\n","    half_dim = embedding_dim // 2\n","    emb = math.log(max_positions) / (half_dim - 1)\n","    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=timesteps.device) * -emb)\n","    emb = timesteps.float()[:, None] * emb[None, :]\n","    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n","    if embedding_dim % 2 == 1:\n","        emb = F.pad(emb, (0, 1), mode='constant')\n","    return emb\n","                                  \n","# Class to embed timestep information into network inputs\n","class TimestepEmbedding(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, output_dim, act=nn.ReLU()):\n","        super().__init__()\n","        self.fc1 = nn.Linear(embedding_dim, hidden_dim)  # First layer: embedding to hidden dimension\n","        self.act = act  # Activation function\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)  # Second layer: hidden dimension to output\n","\n","    def forward(self, t):\n","        if t.dim() == 1:\n","            t = t.unsqueeze(-1)\n","        t = t.float()  # Ensure t is of type float\n","        t = self.fc1(t)\n","        t = self.act(t)\n","        t = self.fc2(t)\n","        return t\n","    \n","# Initialize network weights using a specific strategy for better training performance\n","def init_weights(net, init_gain=0.02, debug=False):\n","    def init_func(m):  # define the initialization function\n","        classname = m.__class__.__name__\n","        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n","            if debug:\n","                print(classname)  # Print class name during debugging\n","            init_gain = 0.02\n","            init.normal_(m.weight.data, 0.0, init_gain)\n","            if hasattr(m, 'bias') and m.bias is not None:\n","                init.constant_(m.bias.data, 0.0)\n","        elif classname.find('LayerNorm') != -1:\n","            init.normal_(m.weight.data, 1.0, init_gain)\n","            init.constant_(m.bias.data, 0.0)\n","\n","    net.apply(init_func)  # apply the initialization function <init_func>\n","\n","# Set up network for use, optionally initialize weights, and set GPU configuration if available\n","def init_net(net, init_gain=0.02, gpu_ids=[], debug=False, initialize_weights=True):\n","    if initialize_weights:\n","        init_weights(net, init_gain=init_gain, debug=debug)\n","    return net\n","\n","# Module to normalize tensors based on a power rule, useful for data and feature normalization\n","class Normalize(nn.Module):\n","    def __init__(self, power=2):\n","        super(Normalize, self).__init__()\n","        self.power = power\n","\n","    def forward(self, x):\n","        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n","        out = x.div(norm)\n","        return out\n","    \n","# Function to convert normalized image data back to standard image format\n","def denormalize(tensor):\n","    return tensor.mul(0.5).add(0.5)  # Converts from [-1, 1] to [0, 1]\n","\n","# Visualize a batch of images using a grid layout\n","def visualize_images(images, title=\"Generated Images\"):\n","    images = images.cpu()  # Move images to CPU for visualization\n","    images = denormalize(images)  # Denormalize images to bring them to displayable format\n","    grid = vutils.make_grid(images, padding=2, normalize=True)  # Create a grid of images\n","    plt.figure(figsize=(8, 8))\n","    plt.axis(\"off\")\n","    plt.title(title)\n","    plt.imshow(np.transpose(grid, (1, 2, 0)))  # Display images\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Define the Generator"]},{"cell_type":"markdown","metadata":{},"source":["#### The generator used in the diffusion model is designed with a focus on conditional image synthesis, where conditions can be temporal (time) or style-based (z). The architecture is structured around ResNet blocks that have been adapted to conditionally modify their processing based on external inputs. \n","\n","#### Below is an overview of the helper components of the generator and of their roles:\n","\n","#### - **Adaptive Layer**: This module adjusts the input feature maps using a style vector z. It computes scaling and bias terms (gamma and beta) from the style vector, which modulates the feature maps to produce style-specific effects.\n","\n","#### - **Conditional ResNet Block**: Each ResNet block in the generator is designed to accept additional inputs—temporal embeddings and style vectors—that influence the block's operation, making it sensitive to both the progression of the diffusion process and the desired output characteristics.\n","\n","#### - **Temporal and Style Embeddings**: These components transform raw time steps and style vectors into formats suitable for integration into the ResNet blocks, ensuring that the generator's output is appropriately varied based on the input conditions.\n","\n","#### This architecture allows the model to generate images that are not only high in quality but also specifically tailored to the conditions provided, making it highly versatile for tasks that require dynamic and context-sensitive image synthesis."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Generator's helper functions\n","\n","class AdaptiveLayer(nn.Module):\n","    # Initializer for the adaptive layer which applies learned affine transformations.\n","    def __init__(self, in_channel, style_dim):\n","        super().__init__()\n","        self.style = nn.Linear(style_dim, in_channel * 2)  # Creates a linear transformation for style codes\n","        # Initialize the affine transform parameters gamma to 1 (scale) and beta to 0 (shift)\n","        self.style.bias.data[:in_channel] = 1\n","        self.style.bias.data[in_channel:] = 0\n","\n","    # Forward pass which applies the affine transformation to the input features\n","    def forward(self, input, style):\n","        gamma, beta = self.style(style).chunk(2, 1)  # Split style into gamma and beta components\n","        gamma, beta = gamma.unsqueeze(2).unsqueeze(3), beta.unsqueeze(2).unsqueeze(3)  # Adjust dimensions for feature map\n","        return gamma * input + beta  # Apply the affine transformation\n","\n","\n","\n","class ResnetBlockCond(nn.Module):\n","    # Initializer for a conditional ResNet block which integrates time and style-based conditioning.\n","    def __init__(self, dim, norm_layer, temb_dim, z_dim):\n","        super(ResnetBlockCond, self).__init__()\n","        self.conv_block = nn.Sequential(\n","            nn.ReflectionPad2d(1),  # Padding for maintaining spatial dimensions after convolution\n","            nn.Conv2d(dim, dim, kernel_size=3, padding=0),  # Standard convolution layer\n","            norm_layer(dim),  # Normalization layer\n","            nn.ReLU(inplace=False)  # Activation function\n","        ) \n","        \n","        self.adaptive = AdaptiveLayer(dim, z_dim)  # Style-based adaptive layer\n","        \n","        self.conv_fin = nn.Sequential(\n","            nn.ReflectionPad2d(1),  # Additional padding for final convolution\n","            nn.Conv2d(dim, dim, kernel_size=3, padding=0),  # Final convolution layer\n","            norm_layer(dim)  # Final normalization layer\n","        )\n","        self.dense_time = nn.Linear(temb_dim, dim)  # Linear layer for transforming time conditioning\n","        nn.init.zeros_(self.dense_time.bias)  # Initialize the bias for the time conditioning layer to zero\n","        self.style = nn.Linear(z_dim, dim * 2)  # Style transformation similar to the adaptive layer\n","        # Initialize gamma to 1 and beta to 0 for the style conditioning\n","        self.style.bias.data[:dim] = 1\n","        self.style.bias.data[dim:] = 0\n","\n","    # Forward pass through the ResNet block with conditional inputs\n","    def forward(self, x, time_cond, z):\n","        time_input = self.dense_time(time_cond)  # Apply linear transformation to the time conditioning\n","        out = self.conv_block(x)  # Pass input through the convolutional block\n","        out = out + time_input[:, :, None, None]  # Add time conditioning to the features\n","        out = self.adaptive(out, z)  # Apply adaptive styling\n","        out = self.conv_fin(out)  # Final convolution to refine features\n","        out = x + out  # Add skip connections for better gradient flow\n","        return out"]},{"cell_type":"markdown","metadata":{},"source":["#### The ResNet Generator with Conditional Blocks is meticulously designed to perform image synthesis, adapting dynamically to temporal and style-based conditioning factors. Here’s a breakdown of the architecture and its core components.\n","\n","#### The generator commences with an **initial convolution layer** that employs a 7x7 kernel to expand the input image's channel dimensions. This layer is equipped with reflection padding to avoid border effects and maintain the integrity of image features.\n","\n","#### Following the initial expansion, the generator employs a series of **convolutional layers** that progressively decrease the spatial dimensions while simultaneously increasing the depth of feature maps. This **downsampling** is achieved through stride-2 convolutions, which effectively reduce the image size by half with each layer, thereby deepening the network’s ability to abstract higher-level features from the input. This mechanism enhances the model's capability to capture complex and abstract representations that are crucial for effective image synthesis.\n","\n","#### The core of the generator is **a series of ResNet blocks**, specifically tailored to accommodate external conditioning. Each block is designed to perform two key functions: process the incoming features to refine and transform them, and integrate external conditional inputs that influence these transformations.\n","#### Within each ResNet block, external conditions are integrated in two forms. First, **temporal embeddings**, which provide cues related to the progression or stages of image generation, are incorporated. These embeddings influence the block operations, tailoring the transformations to specific times in the generative process. Secondly, **style vectors** are utilized to modulate the activations directly, allowing the block to adjust its behavior based on desired style attributes, such as texture or overall appearance.\n","#### Each ResNet block modifies the feature maps by first applying standard convolutional operations followed by the injection of the conditioned biases from both temporal and style-based inputs. This is facilitated by an **adaptive mechanism** that scales and shifts the feature maps accordingly, ensuring that each block’s output is a direct function of both the input features and the external conditions.\n","\n","#### To construct the final image, the generator reverses the dimensionality reduction from the downsampling stages through a series of transposed convolutional layers. These layers gradually restore the spatial dimensions of the feature maps, refining and **upscaling** them back to the original input size. This stage is critical as it reconstructs detailed and high-resolution output from the abstracted feature representations formed in the deeper layers of the network.\n","#### The last part of the upsampling pathway employs a final convolution with a 7x7 kernel, again using reflection padding to enhance image quality. This layer is followed by a Tanh activation function, which normalizes the output pixel values to a standard range, typically between -1 and 1, making the output ready for display or further processing."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ResnetGenerator_cond(nn.Module):\n","    # Initialization of the conditional ResNet generator\n","    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, n_blocks=9):\n","        super(ResnetGenerator_cond, self).__init__()\n","        \n","        # Ensuring the number of blocks is non-negative\n","        assert(n_blocks >= 0)\n","        # Determine if bias is needed based on the type of normalization layer\n","        if type(norm_layer) == functools.partial:\n","            use_bias = norm_layer.func == nn.InstanceNorm2d\n","        else:\n","            use_bias = norm_layer == nn.InstanceNorm2d\n","            \n","        # Initial convolution module to process input image\n","        self.model = nn.Sequential(\n","            nn.ReflectionPad2d(3),  # Padding before initial convolution\n","            nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),  # Initial convolution to transform input channel\n","            norm_layer(ngf),  # Normalization layer\n","            nn.ReLU(inplace=False)  # Activation function\n","        )\n","        \n","        self.ngf = ngf  # Number of generator filters\n","        \n","        # List of residual blocks with conditional inputs\n","        self.model_res = nn.ModuleList([])\n","        # Downsampling part of the model\n","        self.model_downsample = nn.Sequential(\n","            nn.Conv2d(ngf, ngf * 4, kernel_size=3, stride=2, padding=1, bias=use_bias),\n","            norm_layer(ngf * 4),\n","            nn.ReLU(inplace=False)\n","        )\n","        \n","        # Add multiple ResnetBlockCond instances for intermediate processing\n","        for i in range(n_blocks):\n","            self.model_res += [ResnetBlockCond(ngf * 4, norm_layer, temb_dim=4 * ngf, z_dim=4 * ngf)]\n","       \n","        # Upsampling part of the model to restore original image size\n","        self.model_upsample = nn.Sequential(\n","            nn.ConvTranspose2d(ngf * 4, ngf, kernel_size=3, stride=2, padding=1, output_padding=1, bias=use_bias),\n","            norm_layer(ngf),\n","            nn.ReLU(inplace=False),\n","            nn.ReflectionPad2d(3),\n","            nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0),\n","            nn.Tanh()  # Output activation to ensure output values are between -1 and 1\n","        )\n","        \n","        # Define a transformation for the latent vector z\n","        mapping_layers = [PixelNorm(),\n","                          nn.Linear(self.ngf * 4, self.ngf * 4),\n","                          nn.LeakyReLU(0.2)]\n","        self.z_transform = nn.Sequential(*mapping_layers)\n","        \n","        # Time embedding layers\n","        modules_emb = [nn.Linear(self.ngf, self.ngf * 4)]\n","        nn.init.zeros_(modules_emb[-1].bias)  # Initialize the bias to zero for stability\n","        modules_emb += [nn.LeakyReLU(0.2), nn.Linear(self.ngf * 4, self.ngf * 4)]\n","        nn.init.zeros_(modules_emb[-1].bias)  # Again, initialize the bias to zero\n","        modules_emb += [nn.LeakyReLU(0.2)]\n","        self.time_embed = nn.Sequential(*modules_emb)\n","                                \n","    # Define the forward pass with conditional inputs time_cond and z\n","    def forward(self, x, time_cond, z):\n","        z_embed = self.z_transform(z)  # Transform z before feeding it to the ResNet blocks\n","        temb = get_timestep_embedding(time_cond, self.ngf)  # Embedding the time steps\n","        time_embed = self.time_embed(temb)  # Applying the time embedding\n","        out = self.model(x)  # Initial processing of input\n","        out = self.model_downsample(out)  # Apply downsampling\n","        for layer in self.model_res:  # Apply each ResNet block sequentially\n","            out = layer(out, time_embed, z_embed)\n","        out = self.model_upsample(out)  # Final upsampling and output layer\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Initialize the Generator\n","gen = ResnetGenerator_cond(input_nc=3, output_nc=3, ngf=64, n_blocks=9, norm_layer=nn.InstanceNorm2d).to(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Discriminator Architecture\n","#### The discriminator in a diffusion model plays a crucial role in distinguishing generated images from real ones, thus guiding the generator towards producing more realistic images. The discriminator typically employs a series of convolutional layers that progressively downsample the input image, extracting increasingly abstract features that are crucial for making the distinction between real and fake images, making it adapt at handling dynamic scenarios where the state or style of the images changes over time.\n","\n","#### The Key Components of our Discriminator are:\n","\n","#### 1. **Conditional Convolution Block** (ConvBlock_cond): Each block in the discriminator is designed to process input features conditionally based on the temporal embedding. This block integrates convolution, normalization, activation, and optional downsampling while adjusting its processing based on the embedded temporal information.\n","\n","#### 2. **Temporal Embedding Transformation**: Before being fed into the convolution blocks, the temporal embeddings are transformed through a dedicated embedding module (TimestepEmbedding). This module adapts the raw embeddings to have a suitable dimensionality and format for integration into the convolution blocks.\n","\n","#### Here's an overview of the complete architecture:\n","\n","#### - **Initial Layer**: The first layer expands the input channel dimensions while incorporating the first level of conditional processing.\n","#### - **Intermediate Layers**: A series of convolution blocks increase the depth of feature maps progressively, each conditioned on the transformed temporal embeddings. These layers refine the discriminator's capability to extract relevant features for authenticity determination.\n","#### - **Final Layer**: The last convolution block outputs a single channel feature map without downsampling, culminating in a final convolution that determines the real or fake classification.\n","\n","#### This architecture ensures that the discriminator not only effectively discriminates between real and fake images but also adapts its behavior based on the additional context provided by the temporal embeddings, enhancing its relevance and effectiveness in scenarios involving temporal dynamics."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# DISCRIMINATOR\n","class ConvBlock_cond(nn.Module):\n","    \"\"\"Conditional convolution block with embedding integration for discriminator.\"\"\"\n","    def __init__(self, in_channels, out_channels, embedding_dim, kernel_size=3, stride=1, padding=1, use_bias=True, norm_layer=nn.BatchNorm2d, downsample=True):\n","        super(ConvBlock_cond, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=use_bias) # Standard convolution layer\n","        self.norm = norm_layer(out_channels) # Normalization layer specified by norm_layer argument\n","        self.act = nn.LeakyReLU(0.2, inplace=True) # Activation function set to LeakyReLU for stable gradients\n","        self.downsample = downsample # Option to downsample the feature map for reducing spatial dimensions\n","        self.dense = nn.Linear(embedding_dim, out_channels) # Linear layer to transform the embedding dimension to match output channels\n","\n","    def forward(self, x, t_emb):\n","        out = self.conv(x) # Apply convolution to the input\n","        out = out + self.dense(t_emb)[..., None, None] # Add transformed timestep embedding to the convolution output\n","        out = self.norm(out) # Normalize the output\n","        out = self.act(out) # Apply the activation function\n","        # Conditionally apply downsampling\n","        if self.downsample:\n","            out = nn.functional.avg_pool2d(out, kernel_size=2, stride=2)\n","        return out\n","\n","class NLayerDiscriminator_ncsn_new(nn.Module):\n","    \"\"\"Discriminator that uses conditional convolution blocks to process input images.\"\"\"\n","    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n","        \"\"\"Initialize the discriminator with conditional convolution blocks.\"\"\"\n","        super(NLayerDiscriminator_ncsn_new, self).__init__()\n","        # Determine if bias should be used based on the type of normalization layer\n","        use_bias = norm_layer == nn.InstanceNorm2d\n","\n","        # List of modules that make up the main discriminator model\n","        self.model_main = nn.ModuleList()\n","        \n","        # First convolution block that processes the initial input layer\n","        self.model_main.append(\n","            ConvBlock_cond(input_nc, ndf, 4 * ndf, kernel_size=4, stride=1, padding=1, use_bias=use_bias))\n","\n","        # Dynamically add intermediate convolution blocks with increasing feature depth\n","        nf_mult = 1\n","        for n in range(1, n_layers):\n","            nf_mult_prev = nf_mult\n","            nf_mult = min(2 ** n, 8)\n","            self.model_main.append(\n","                ConvBlock_cond(ndf * nf_mult_prev, ndf * nf_mult, 4 * ndf, kernel_size=4, stride=1, padding=1, use_bias=use_bias, norm_layer=norm_layer)\n","            )\n","\n","        # Add the last convolution block without downsampling to maintain spatial dimensions\n","        nf_mult_prev = nf_mult\n","        nf_mult = min(2 ** n_layers, 8)\n","        self.model_main.append(\n","            ConvBlock_cond(ndf * nf_mult_prev, ndf * nf_mult, 4 * ndf, kernel_size=4, stride=1, padding=1, use_bias=use_bias, norm_layer=norm_layer, downsample=False)\n","        )\n","        \n","        # Final convolution layer that outputs a single channel for discrimination\n","        self.final_conv = nn.Conv2d(ndf * nf_mult, 1, kernel_size=4, stride=1, padding=1)\n","        # Time embedding layer that prepares the timestep embedding for integration into convolution blocks\n","        self.t_embed = TimestepEmbedding(\n","            embedding_dim=1,\n","            hidden_dim=4 * ndf,\n","            output_dim=4 * ndf,\n","            act=nn.LeakyReLU(0.2)\n","        )\n","\n","    def forward(self, input, t_emb, input2=None):\n","        \"\"\"Forward pass through the discriminator with optional dual inputs and timestep embedding.\"\"\"\n","        t_emb = t_emb.float()  # Convert timestep embedding to float for processing\n","        t_emb = self.t_embed(t_emb)  # Apply embedding transformation\n","        # If a second input is provided, concatenate it with the first input\n","        out = torch.cat([input, input2], dim=1) if input2 is not None else input\n","        \n","        # Process each convolution block with the current output and timestep embedding\n","        for layer in self.model_main:\n","            out = layer(out, t_emb) if isinstance(layer, ConvBlock_cond) else layer(out)\n","        \n","        return self.final_conv(out)  # Apply the final convolution layer to produce the discriminator's output"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Initialize the discriminator\n","disc = NLayerDiscriminator_ncsn_new(input_nc=3, ndf=64, n_layers=3, norm_layer=nn.InstanceNorm2d).to(device)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"056e30e8-658f-4189-b032-4b205443e399","_uuid":"91914a6b-233b-4cec-b67f-af711eaec8ce","trusted":true},"source":["## Define netE and netF"]},{"cell_type":"markdown","metadata":{},"source":["#### Our architecture is composed of other two networks, respectively called netF and netE. \n","#### - **NetF** : It is an instance of the PatchSampleF class, whose aim is to extract and processes patches from the feature maps generated by other parts of the model. The extracted patches are normalized to have a unit norm, which helps in stabilizing the training and ensuring consistent scaling. If specified, the extracted patches are projected into a lower-dimensional space through an MLP, which can potentially highlight important feature relationships. We will see later that it is used to compute the Noise Contrastive Estimation (NCE) Loss, which helps in aligning features from the source and target domains, improving the quality and consistency of generated images.\n","#### - **NetE** : On the other side, netE is another discriminator. Its input consists of concatenated images (noisy input and generated output). As we will see later, it is fundamental in computing the loss associated with bridging the gap between two given distributions, aligning distributions, regularizing entropy, and guiding the optimization process towards finding the optimal joint distribution."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# PatchSampleF aims to extract and processes patches from the feature maps generated by other parts of the model\n","\n","class PatchSampleF(nn.Module):\n","    \"\"\" PatchsampleF is a class designed to sample and normalize patches from feature maps. \n","    It can optionally use multi-layer perceptrons (MLPs) for further processing. During the forward pass, \n","    it selects a specified number of patches from each feature map, optionally processes them through MLPs, \n","    and applies L2 normalization. It also supports specifying custom patch indices or randomly selecting them if not provided \"\"\"\n","    def __init__(self, use_mlp=False, init_type='normal', init_gain=0.02, nc=256, gpu_ids=[]):\n","        super(PatchSampleF, self).__init__()\n","        self.l2norm = Normalize(2)  \n","        self.use_mlp = use_mlp\n","        self.nc = nc\n","        self.mlp_init = False\n","        self.init_type = init_type\n","        self.init_gain = init_gain\n","        self.gpu_ids = gpu_ids\n","\n","    def create_mlp(self, feats):\n","        \"\"\" Creation of MLP for further processing of features and patches \"\"\"\n","        for mlp_id, feat in enumerate(feats):\n","            input_nc = feat.shape[0]\n","            mlp = nn.Sequential(\n","                nn.Linear(input_nc, self.nc),\n","                nn.LeakyReLU(0.2),\n","                nn.Linear(self.nc, self.nc)\n","            )\n","            mlp.cuda()\n","            setattr(self, f'mlp_{mlp_id}', mlp)\n","        self.mlp_init = True\n","\n","    def forward(self, feats, num_patches=64, patch_ids=None):\n","        \"\"\" The forward method returns the sampled (and possibly processed) patches and their indices. \n","        If no patches are sampled, it reshapes the feature maps back to their original dimensions \"\"\"\n","        if self.use_mlp and not self.mlp_init:\n","            self.create_mlp(feats)\n","\n","        return_feats = []\n","        return_ids = []\n","\n","        for feat_id, feat in enumerate(feats):\n","            # Add batch dimension if missing\n","            if len(feat.shape) == 3:\n","                feat = feat.unsqueeze(0)\n","\n","            B, C, H, W = feat.shape\n","            feat_reshape = feat.permute(0, 2, 3, 1).reshape(B, -1, C)  # Reshape to [B, H*W, C]\n","\n","            if num_patches > 0:\n","                if patch_ids is not None and len(patch_ids) > feat_id:\n","                    current_patch_ids = patch_ids[feat_id]\n","                else:\n","                    # Generate random patch indices if none provided\n","                    current_patch_ids = [torch.randperm(feat_reshape.shape[1])[:num_patches].to(feat.device) for _ in range(B)]\n","                current_patch_ids = [torch.tensor(pid, dtype=torch.long, device=feat.device) for pid in current_patch_ids]\n","                # Sampling patches\n","                x_sample = torch.cat([feat_reshape[b, pid, :] for b, pid in enumerate(current_patch_ids)], dim=0)\n","                return_ids.append(current_patch_ids)\n","            else:\n","                x_sample = feat_reshape.reshape(-1, C)\n","                current_patch_ids = [torch.tensor([], dtype=torch.long, device=feat.device) for _ in range(B)]\n","                return_ids.append(current_patch_ids)\n","\n","            if self.use_mlp:\n","                mlp = getattr(self, f'mlp_{feat_id}')\n","                x_sample = mlp(x_sample)\n","\n","            x_sample = self.l2norm(x_sample)\n","\n","            return_feats.append(x_sample)\n","\n","        # Since we add patches for each batch, we must handle the concatenation properly\n","        if num_patches == 0:\n","            return_feats = [f.view(B, H, W, -1).permute(0, 3, 1, 2) for f in return_feats]\n","\n","        return return_feats, return_ids "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# define netF and netE\n","netF = PatchSampleF(use_mlp=True, init_type='normal', init_gain=0.02, nc=256).to(device)\n","netE = NLayerDiscriminator_ncsn_new(input_nc=3*4, ndf=64, n_layers=3, norm_layer=nn.InstanceNorm2d).to(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Loss Criterions "]},{"cell_type":"markdown","metadata":{},"source":["#### Before delving deeply inside the model, we define two loss criterions\n","* #### **Cross Entropy Loss (NCE)** : Used for contrastive learning tasks, calculating loss for feature differences across network layers.\n","* #### **Mean Squared Error Loss (Gan Loss)** : Applied in adversarial training, measuring the difference between real and generated samples. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define Loss Criterions\n","def criterionNCE(nce_layers):\n","    criterionNCE = []\n","    for nce_layer in nce_layers:\n","        criterionNCE.append(nn.CrossEntropyLoss(reduction='none').to(device))\n","    return criterionNCE\n","\n","def criterionGAN():\n","    return nn.MSELoss().to(device)\n","\n","\n","class GANLoss(nn.Module):\n","    \"\"\"Define Least Squares GAN loss.\"\"\"\n","\n","    def __init__(self, target_real_label=1.0, target_fake_label=0.0):\n","        \"\"\" Initialize the GANLoss class, whose parameters are float representing labels for real and fake images \"\"\"\n","        \n","        super(GANLoss, self).__init__()\n","        self.register_buffer('real_label', torch.tensor(target_real_label))\n","        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n","        self.loss = nn.MSELoss()\n","\n","    def get_target_tensor(self, prediction, target_is_real):\n","        \"\"\" Creates label tensors filled with the ground truth label, and with the size of the input \"\"\"\n","        if target_is_real:\n","            target_tensor = self.real_label\n","        else:\n","            target_tensor = self.fake_label\n","        return target_tensor.expand_as(prediction)\n","\n","    def __call__(self, prediction, target_is_real):\n","        \"\"\" Calculate loss given Discriminator's prediction and ground truth labels \"\"\"\n","        target_tensor = self.get_target_tensor(prediction, target_is_real)\n","        return self.loss(prediction, target_tensor)"]},{"cell_type":"markdown","metadata":{},"source":["## SB Model"]},{"cell_type":"markdown","metadata":{},"source":["#### At this point, we have described all the architectures needed for our Diffusion Model. Let's have some fun! :)\n","\n","#### Backbone Structure (**forward function**) : \n","* #### The diffusion process gradually adds noise to the images, which is crucial for generating diverse and realistic intermediate states that the generator network refines\n","* #### By interpolating between previous states and adding controlled noise, we ensure smooth transitions and avoid abrupt changes in the image states \n","* #### Using the generator network at each timestep allows us to iteratively improve the quality of the noisy images, aligning them closer to the desired output distribution \n"]},{"cell_type":"markdown","metadata":{},"source":["#### The losses computed are : \n","\n","1. #### ***D_loss*** : Discriminator D loss is computed through the compute_D_loss method, which calculates the adversarial loss for the discriminator. It computes separate losses for real and fake images and then combines them to obtain the total discriminator loss. The latter is scaled by 0.5 to ensure equal contribution from both fake and real losses. This loss guides the training of the discriminator to better distinguish between real and generated images.\n","2. #### ***E_loss*** : Discriminator E loss is computed through the compute_E_loss method, whose primary goal is to guide the training of netE towards learning meaningful representations of transition distributions between noisy and generated image pairs. By minimizing the loss and incorporating regularization techniques, the network aims to align these distributions effectively, facilitating the generation of realistic and coherent images by the generator network G\n","3. #### ***G_loss*** : Generator G loss is computed through the compute_G_loss method, that evaluates the overall loss incurred by, encompassing multiple loss components : \n","    * #### G_GAN Loss : Encourages the generator to produce realistic images \n","    * #### Schrödinger Bridge loss : Ensures temporal consistency and distribution alignment \n","    * #### Noise Contrastive Estimation : Promotes feature alignment between real and generated images \n","\n","  #### This combination enables the generator to learn effective image generation strategies that produce high-quality images consistent with the target distribution.\n","4. #### ***NCE_loss*** : The idea behind the compute_NCE_loss method is to compute a form of contrastive loss. It aims to learn representations by contrasting similarities and differences between features extracted from the source and target images at multiple layers of the network. After applying a weighting factor, it returns the average loss across all layers"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class SBModel(nn.Module):\n","    \n","    def __init__(self):\n","        \"\"\" Initializes the SBModel class, setting up parameters, loss names, model names, visual names, optimizers, and other necessary configurations \"\"\"\n","        # Note that parameters have been taken directly from the paper, except for the number of epochs, due to our hardware computation limits \n","        super(SBModel,self).__init__()\n","        self.loss_names = ['G_GAN', 'D_real', 'D_fake', 'G', 'NCE','SB']\n","        self.model_names = ['G','F','D','E']\n","        self.visual_names = ['real_A','real_A_noisy', 'fake_B', 'real_B']\n","        self.optimizers = []\n","        self.tau = 0.1\n","        self.device = device   \n","        self.lambda_NCE = 1.0 \n","        self.nce_idt = True\n","        self.nce_layers = [0,4,8,12,16]  \n","        self.num_patches = 256\n","        self.netG = gen\n","        self.netD = disc\n","        self.netE = netE\n","        self.netF = netF \n","        self.ngf = 64\n","        self.criterionNCE = criterionNCE(self.nce_layers)\n","        self.criterionGAN = GANLoss().to(device)\n","        self.lr = 0.00001\n","        self.beta1 = 0.5\n","        self.beta2 = 0.999 \n","        \n","        # Defining Optimizers\n","        self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=self.lr, betas=(self.beta1, self.beta2))\n","        self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=self.lr, betas=(self.beta1, self.beta2))\n","        self.optimizer_E = torch.optim.Adam(self.netE.parameters(),lr=self.lr, betas=(self.beta1, self.beta2))\n","        \n","    def data_dependent_initialize(self, dataA,dataB, dataA2, dataB2): \n","        \"\"\" Prepares the model for training, computing fake images using the generator and initial losses for the generator 'G' \n","        and the discriminators 'D' and 'E'. It is conditioned on whether the loss function involving the NCE term is active. \n","        If so, it initializes an optimizer for netF \"\"\"\n","        bs = 1\n","        self.set_input(dataA,dataB, dataA2, dataB2)\n","        self.real_A = self.real_A[:bs]\n","        self.real_B = self.real_B[:bs]\n","        self.real_A2 = self.real_A2[:bs]\n","        self.real_B2 = self.real_B2[:bs]\n","        self.forward()  \n","        self.compute_G_loss().backward()\n","        self.compute_D_loss().backward()\n","        self.compute_E_loss().backward()  \n","        if self.lambda_NCE > 0.0:\n","            self.optimizer_F = torch.optim.Adam(self.netF.parameters(), lr=self.lr, betas=(self.beta1, self.beta2))\n","            self.optimizers.append(self.optimizer_F)\n","        \n","    \n","    def set_input(self, dataA, dataB, dataA2, dataB2):\n","        \"\"\" Responsible for unpacking input data from the dataloader and performing any necessary preprocessing steps \"\"\"\n","        self.real_A = dataA.to(device)\n","        self.real_B = dataB.to(device)\n","        self.real_A2 = dataA2.to(device)\n","        self.real_B2 = dataB2.to(device)\n","        \n","    def set_requires_grad(self, nets, requires_grad=True):\n","        \"\"\" Toggles the requirement for gradient computation for the parameters in the provided networks. \n","        It is s helpful for controlling which parts of the model are trainable during different training phases \"\"\"\n","        if not isinstance(nets, list):\n","            nets = [nets]\n","        for net in nets:\n","            if net is not None:\n","                for param in net.parameters():\n","                    param.requires_grad = requires_grad\n","    \n","    def get_current_losses(self):\n","        \"\"\" Retrieves the current training losses/errors from the model and returns them as a dictionary \"\"\"\n","        errors_ret = {}\n","        for name in self.loss_names:\n","            if isinstance(name, str):\n","                errors_ret[name] = float(getattr(self, 'loss_' + name))  # float(...) works for both scalar tensor and float number\n","        return errors_ret\n","        \n","    def optimize_parameters(self):\n","        \"\"\" It ensures that the gradients are properly calculated and used to update the network weights during training \"\"\"\n","        \n","        # Forward pass\n","        self.forward()\n","        \n","        # Set models to training mode\n","        self.netG.train()\n","        self.netE.train()\n","        self.netD.train()\n","        self.netF.train()\n","        \n","        # Update Discriminator D \n","        self.set_requires_grad(self.netD, True) # Enables gradient calculation for D\n","        self.optimizer_D.zero_grad()  # Zeros the gradients of D_optimizer\n","        self.loss_D = self.compute_D_loss()  # Computes D loss \n","        self.loss_D.backward()  # Backpropagates the gradient \n","        torch.nn.utils.clip_grad_norm_(self.netD.parameters(), max_norm=1)   # Clip gradients to avoid exploiding gradients \n","        self.optimizer_D.step()  # Updates the parameters of D \n","        \n","        # Update Discriminator E\n","        self.set_requires_grad(self.netE, True)  # Enables gradient calculation for E\n","        self.optimizer_E.zero_grad()  # Zeros the gradients of E_optimizer\n","        self.loss_E = self.compute_E_loss()  # Computes E loss\n","        self.loss_E.backward()   # Backpropagates the gradient \n","        torch.nn.utils.clip_grad_norm_(self.netE.parameters(), max_norm=1)  # Clip gradients to avoid exploiding gradients \n","        self.optimizer_E.step()  # Updates the parameters of E\n","    \n","        # Update Generator G\n","        self.set_requires_grad(self.netD, False)  # Disables gradient calculation for discriminator D since it is not being updated in this step  \n","        self.set_requires_grad(self.netE, False)  # Disables gradient calculation for discriminator E since it is not being updated in this step  \n","        \n","        self.optimizer_G.zero_grad()  # Zeros the gradient of G_optimizer\n","        self.optimizer_F.zero_grad()  # Zeros the gradient of F_optimizer \n","        \n","        self.loss_G = self.compute_G_loss()   # Compute G loss \n","        self.loss_G.backward()  # Backpropagates the gradient\n","        \n","        torch.nn.utils.clip_grad_norm_(self.netG.parameters(), max_norm=1)  # Clip gradients to avoid exploiding gradients \n","        self.optimizer_G.step()  # Updates the parameters of G\n","    \n","        torch.nn.utils.clip_grad_norm_(self.netF.parameters(), max_norm=1)  #  Clip gradients to avoid exploiding gradients  \n","        self.optimizer_F.step()  # Updates the parameters of F\n","\n","    def forward(self):\n","        \"\"\" Diffusion Process, described above \"\"\"\n","        tau = 0.01  # Entropy parameter \n","        T = 5  # Number of time steps \n","        incs = np.array([0] + [1/(i+1) for i in range(T-1)])  # Array of incremental values used to define time steps \n","        times = np.cumsum(incs)\n","        times = times / times[-1]\n","        times = 0.5 * times[-1] + 0.5 * times\n","        times = torch.tensor(times).float().cuda()   # Array of normalized time steps, scaled and shifted \n","        self.times = times\n","        bs =  self.real_A.size(0)\n","        time_idx = (torch.randint(T, size=[1]).cuda() * torch.ones(size=[1]).cuda()).long()  # Randomly selected time step index \n","        self.time_idx = time_idx  \n","        self.timestep     = times[time_idx]  # Actual time value corresponsing to 'time_idx'\n","        \n","        with torch.no_grad():\n","            self.netG.eval()\n","            for t in range(0, self.time_idx.int().item() + 1):  # Iteration over each time step up to the current index \n","                if t > 0:\n","                    # Interpolation factors based on the current and previous time steps for temporal interpolation -> Paper Fig. 3 \n","                    delta = times[t] - times[t-1]   \n","                    denom = times[-1] - times[t-1]  \n","                    inter = (delta / denom)         \n","                    scale = (delta * (1 - delta / denom))  \n","                    \n","                \n","                \"\"\" Handling Input 1 \"\"\"\n","                Xt       = self.real_A if (t == 0) else (1-inter)* Xt + inter * Xt_1.detach() + (scale * tau).sqrt() * torch.randn_like(Xt).to(self.real_A.to(device))\n","                # Xt is updated using its previous state, the output from the previous timestep Xt_1, and added Gaussian noise \n","                time_idx = (t * torch.ones(size=[self.real_A.shape[0]]).to(self.real_A.to(device))).long()\n","                time     = times[time_idx]\n","                z        = torch.randn(size=[self.real_A.shape[0],4*self.ngf]).to(self.real_A.to(device))\n","                Xt_1     = self.netG(Xt, time_idx, z) # Xt_1 is the output of the generator given the noisy input Xt, current time_idx, and latent vector z. \n","        \n","                \"\"\" Handling input 2 \"\"\"\n","                # We consider another input to help stabilize the training process. This ensures that the model learns consistent features across different instances. It can be considered as a sort of data augmentation \n","                Xt2 = self.real_A2 if (t == 0) else (1-inter)*Xt2 + inter*Xt_12.detach() + (scale*tau).sqrt() * torch.randn_like(Xt2).to(self.real_A2.to(device))\n","                # Xt2 is updated using its previous state, the output from the previous timestep Xt_12, and added Gaussian noise \n","                time_idx = (t * torch.ones(size=[self.real_A.shape[0]]).to(self.real_A.to(device))).long()\n","                time     = times[time_idx]\n","                z        = torch.randn(size=[self.real_A.shape[0], 4 * self.ngf]).to(self.real_A.to(device))\n","                Xt_12    = self.netG(Xt2, time_idx, z)  # Xt_12 is the output of the generator given the noisy input Xt2, current time_idx, and latent vector z.\n","                \n","                if self.nce_idt:\n","                    XtB = self.real_B if (t == 0) else (1-inter) * XtB + inter * Xt_1B.detach() + (scale * tau).sqrt() * torch.randn_like(XtB).to(self.real_A.to(device))\n","                    # XtB is updated using its previous state, the output from the previous timestep Xt_1B, and added Gaussian noise \n","                    time_idx = (t * torch.ones(size=[self.real_A.shape[0]]).to(self.real_A.to(device))).long()\n","                    time     = times[time_idx]\n","                    z        = torch.randn(size=[self.real_A.shape[0],4*self.ngf]).to(self.real_A.to(device))\n","                    Xt_1B = self.netG(XtB, time_idx, z)  # Xt_1B is the output of the generator given the noisy input XtB, current time_idx, and latent vector z.\n","                    \n","            if self.nce_idt:\n","                self.XtB = XtB.detach()\n","                \n","            self.real_A_noisy = Xt.detach()\n","            self.real_A_noisy2 = Xt2.detach()          \n","        \n","        z_in    = torch.randn(size=[2*bs,4*self.ngf]).to(self.real_A.to(device))  # Random noise for generator inputs \n","        z_in2    = torch.randn(size=[bs,4*self.ngf]).to(self.real_A.to(device))   # # Random noise for generator inputs \n","        \n","        self.real = torch.cat((self.real_A, self.real_B), dim=0) if self.nce_idt else self.real_A   # Concatenate real horse and real zebra image if identity loss is enabled \n","        self.realt = torch.cat((self.real_A_noisy, self.XtB), dim=0) if self.nce_idt else self.real_A_noisy  # Concatenate noisy horse and noisy zebra if identity loss is enabled \n","        self.fake = self.netG(self.realt,self.time_idx,z_in)   # Apply the generator to the concatenated first step of  noisy images \n","        self.fake_B2 =  self.netG(self.real_A_noisy2,self.time_idx,z_in2)  # Apply the generator to the second set of noisy image \n","        self.fake_B = self.fake[:self.real_A.size(0)]  # Extract \"generated zebra\" (horse with zebra's features) from self.fake   \n","\n","    def compute_D_loss(self):\n","        \"\"\" Computation of Discriminator D loss, combining losses for real and fake images \"\"\"\n","        bs = self.real_A.size(0)\n","        fake = self.fake_B.detach()   # Obtained Fake Images \n","        std = torch.rand(size=[1]).item()\n","        pred_fake = self.netD(fake,self.time_idx)    # Discriminator D's predictions for fake images \n","        self.loss_D_fake = self.criterionGAN(pred_fake, False).mean() #Computes Adversarial Loss for fake images, setting target label to 'False' to denote that the images are fake\n","        self.pred_real = self.netD(self.real_B,self.time_idx)  # Discriminator D's predictions for real images \n","        loss_D_real = self.criterionGAN(self.pred_real, True)  # Computes Adversarial Loss for real images, setting target label to 'True' to denote that the images are real\n","        self.loss_D_real = loss_D_real.mean()\n","        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5  # Total Discriminator D loss \n","        return self.loss_D\n","    \n","    def compute_E_loss(self):\n","        \"\"\" Computation of Discriminator E loss, aiming to bridge the gap between distributions \"\"\"\n","        bs =  self.real_A.size(0)\n","        \n","        XtXt_1 = torch.cat([self.real_A_noisy,self.fake_B.detach()], dim=1)   # Concatenation of noisy input image with the corresponding generated image \n","        XtXt_2 = torch.cat([self.real_A_noisy2,self.fake_B2.detach()], dim=1) # Concatenation of noisy input image 2 with the corresponding generated image 2 \n","        temp = torch.logsumexp(self.netE(XtXt_1, self.time_idx, XtXt_2).reshape(-1), dim=0).mean()  # Entropy term which includes a log-sum-exp term for stability\n","        # This operation helps to approximate the log of the integral of the transition probabilities, providing a more stable and robust computation \n","        self.loss_E = -self.netE(XtXt_1, self.time_idx, XtXt_1).mean() +temp + temp**2  # Total E loss is computed, including terms related to negative LL and regularization \n","        \n","        return self.loss_E\n","    \n","    def compute_G_loss(self):\n","        \"\"\" Compute Generator G Loss, given by the combination of G_GAN, SB and NCE loss \"\"\"\n","        bs = 1\n","        tau = 0.01\n","        lambda_GAN = 1.0\n","        lambda_SB = 1.0\n","        lambda_NCE = 1.0\n","        \n","        fake = self.fake_B\n","        std = torch.rand(size=[1]).item() \n","        \n","        if lambda_GAN > 0:\n","            pred_fake = self.netD(fake,self.time_idx)  # Discriminator D predictions on generated images \n","            self.loss_G_GAN = self.criterionGAN(pred_fake, True).mean() # Compares fake predictions to the target label 'True', indicating these should be real \n","        else:\n","            self.loss_G_GAN = 0\n","\n","        self.loss_SB = 0\n","        if lambda_SB > 0:\n","            XtXt_1 = torch.cat([self.real_A_noisy, self.fake_B], dim=1)\n","            XtXt_2 = torch.cat([self.real_A_noisy2, self.fake_B2], dim=1)\n","            bs = 1\n","            ET_XY    = self.netE(XtXt_1, self.time_idx, XtXt_1).mean() - torch.logsumexp(self.netE(XtXt_1, self.time_idx, XtXt_2).reshape(-1), dim=0)  # Helps in aligning the distributions \n","            self.loss_SB = -(self.timestep - self.time_idx[0])/self.timestep*tau*ET_XY\n","            self.loss_SB += self.tau*torch.mean((self.real_A_noisy-self.fake_B)**2)\n","        \n","        if lambda_NCE > 0:\n","            self.loss_NCE = self.calculate_NCE_loss(self.real_A, fake, lambda_NCE) # NCE loss helps in aligning the feature distributions of the real and generated images \n","        else: \n","            self.loss_NCE, self.loss_NCE_bd = 0.0, 0.0\n","\n","        self.loss_G = lambda_GAN * self.loss_G_GAN + lambda_SB * self.loss_SB + lambda_NCE * self.loss_NCE # Total Generator loss \n","        return self.loss_G\n","        \n","    def calculate_NCE_loss(self, src, tgt, lambda_NCE):\n","        \"\"\" Computation of Noise Contrastive Estimation Loss, measuring similarity between patches extracted from source and target images\"\"\"\n","        num_patches = 256\n","        nce_layers = [0,4,8,12,16]\n","        num_layers = len(nce_layers)\n","        z = torch.randn(size=[self.real_A.size(0),4*self.ngf]).to(self.real_A.to(device))\n","        feat_q = self.netG(tgt, self.time_idx, z)  # Feature Map obtained from the generator for target images \n","        feat_k = self.netG(src, self.time_idx,z)   # Feature Map obtained from the generator for source images \n","        feat_k_pool, sample_ids = self.netF(feat_k, num_patches, None)  # Through netF, we extract patches from the feature maps for target images \n","        feat_q_pool, _ = self.netF(feat_q, num_patches, sample_ids)     # Through netF, we extract patches from the feature maps for source images \n","\n","        total_nce_loss = 0.0\n","        for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, self.criterionNCE, nce_layers):\n","            loss = crit(f_q, f_k) * lambda_NCE   # Cross entropy is used to measure the similarity between the patches \n","            total_nce_loss += loss.mean()\n","        return total_nce_loss / num_layers  # Total loss is averaged across all sampled patches and layers  "]},{"cell_type":"markdown","metadata":{},"source":["## Introduction to Inception V3\n","\n","#### Inception v3 is a convolutional neural network pre-trained on a large dataset of images (such as ImageNet) and is used to extract meaningful features from images. \n","\n","#### Using a network like Inception v3 allows for obtaining a high-level representation of images that captures relevant information to evaluate the quality and similarity of generated images compared to real ones. \n","\n","#### Below, we have provided the model that will be used for calculating FID and KID values: "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Inception V3\n","# URL for the pretrained model weights file\n","FID_WEIGHTS_URL = 'https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth'\n","\n","try:\n","    from torchvision.models.utils import load_state_dict_from_url\n","except ImportError:\n","    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n","\n","class InceptionV3(nn.Module):\n","    # Index of the default Inception block to return,\n","    # corresponds to the output of the final average pooling\n","    DEFAULT_BLOCK_INDEX = 3\n","\n","    # Maps feature dimensionality to their respective output block indices\n","    BLOCK_INDEX_BY_DIM = {\n","        64: 0,   # First max pooling features\n","        192: 1,  # Second max pooling features\n","        768: 2,  # Pre-auxiliary classifier features\n","        2048: 3  # Final average pooling features\n","    }\n","\n","    def __init__(self,\n","                 output_blocks=[DEFAULT_BLOCK_INDEX],\n","                 resize_input=True,\n","                 normalize_input=True,\n","                 requires_grad=False,\n","                 use_fid_inception=True):\n","        '''Initialization function'''\n","        super(InceptionV3, self).__init__()\n","\n","        self.resize_input = resize_input\n","        self.normalize_input = normalize_input\n","        self.output_blocks = sorted(output_blocks)\n","        self.last_needed_block = max(output_blocks)\n","\n","        # Ensure that the last needed block is not greater than 3\n","        assert self.last_needed_block <= 3, \\\n","            'Last possible output block index is 3'\n","\n","        self.blocks = nn.ModuleList()\n","\n","        # Load the correct pretrained Inception model\n","        if use_fid_inception:\n","            inception = fid_inception_v3()\n","        else:\n","            inception = _inception_v3(pretrained=True)\n","\n","        # Block 0: input to the first max pooling\n","        block0 = [\n","            inception.Conv2d_1a_3x3,\n","            inception.Conv2d_2a_3x3,\n","            inception.Conv2d_2b_3x3,\n","            nn.MaxPool2d(kernel_size=3, stride=2)\n","        ]\n","        self.blocks.append(nn.Sequential(*block0))\n","\n","        # Block 1: first max pooling to the second max pooling\n","        if self.last_needed_block >= 1:\n","            block1 = [\n","                inception.Conv2d_3b_1x1,\n","                inception.Conv2d_4a_3x3,\n","                nn.MaxPool2d(kernel_size=3, stride=2)\n","            ]\n","            self.blocks.append(nn.Sequential(*block1))\n","\n","        # Block 2: second max pooling to the auxiliary classifier\n","        if self.last_needed_block >= 2:\n","            block2 = [\n","                inception.Mixed_5b,\n","                inception.Mixed_5c,\n","                inception.Mixed_5d,\n","                inception.Mixed_6a,\n","                inception.Mixed_6b,\n","                inception.Mixed_6c,\n","                inception.Mixed_6d,\n","                inception.Mixed_6e,\n","            ]\n","            self.blocks.append(nn.Sequential(*block2))\n","\n","        # Block 3: auxiliary classifier to the final average pooling\n","        if self.last_needed_block >= 3:\n","            block3 = [\n","                inception.Mixed_7a,\n","                inception.Mixed_7b,\n","                inception.Mixed_7c,\n","                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n","            ]\n","            self.blocks.append(nn.Sequential(*block3))\n","\n","        # Set requires_grad for all parameters based on the requires_grad flag\n","        for param in self.parameters():\n","            param.requires_grad = requires_grad\n","\n","    def forward(self, inp):\n","        '''Forward function'''\n","        outp = []\n","        x = inp\n","\n","        # Resize input if the flag is set\n","        if self.resize_input:\n","            x = F.interpolate(x,\n","                              size=(299, 299),\n","                              mode='bilinear',\n","                              align_corners=False)\n","\n","        # Normalize input if the flag is set\n","        if self.normalize_input:\n","            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n","\n","        # Pass through the blocks sequentially\n","        for idx, block in enumerate(self.blocks):\n","            x = block(x)\n","            if idx in self.output_blocks:\n","                outp.append(x)\n","\n","            # Break if the last needed block is reached\n","            if idx == self.last_needed_block:\n","                break\n","\n","        return outp\n","\n","\n","def _inception_v3(*args, **kwargs):\n","    try:\n","        version = tuple(map(int, torchvision.__version__.split('.')[:2]))\n","    except ValueError:\n","        # Just a caution against weird version strings\n","        version = (0,)\n","\n","    if version >= (0, 6):\n","        kwargs['init_weights'] = False\n","\n","    return torchvision.models.inception_v3(*args, **kwargs)\n","\n","\n","def fid_inception_v3():\n","    '''The Inception model for FID computation uses a different set of weights and has a slightly different structure than torchvision's Inception.\n","        This method first constructs torchvision's Inception and then patches the necessary parts that are different in the FID Inception model.'''\n","    inception = _inception_v3(num_classes=1008,\n","                              aux_logits=False,\n","                              pretrained=False)\n","    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)\n","    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)\n","    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)\n","    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)\n","    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)\n","    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)\n","    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)\n","    inception.Mixed_7b = FIDInceptionE_1(1280)\n","    inception.Mixed_7c = FIDInceptionE_2(2048)\n","\n","    # Load the state dictionary for the FID Inception model\n","    state_dict = load_state_dict_from_url(FID_WEIGHTS_URL, progress=True)\n","    inception.load_state_dict(state_dict)\n","    return inception\n","\n","class FIDInceptionA(torchvision.models.inception.InceptionA):\n","    '''InceptionA block patched for FID computation'''\n","    def __init__(self, in_channels, pool_features):\n","        super(FIDInceptionA, self).__init__(in_channels, pool_features)\n","\n","    def forward(self, x):\n","        branch1x1 = self.branch1x1(x)\n","\n","        branch5x5 = self.branch5x5_1(x)\n","        branch5x5 = self.branch5x5_2(branch5x5)\n","\n","        branch3x3dbl = self.branch3x3dbl_1(x)\n","        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n","        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n","\n","        # Patch: Tensorflow's average pool does not use the padded zero's in\n","        # its average calculation\n","        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n","                                   count_include_pad=False)\n","        branch_pool = self.branch_pool(branch_pool)\n","\n","        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n","        return torch.cat(outputs, 1)\n","\n","class FIDInceptionC(torchvision.models.inception.InceptionC):\n","    '''InceptionC block patched for FID computation'''\n","    def __init__(self, in_channels, channels_7x7):\n","        super(FIDInceptionC, self).__init__(in_channels, channels_7x7)\n","\n","    def forward(self, x):\n","        branch1x1 = self.branch1x1(x)\n","\n","        branch7x7 = self.branch7x7_1(x)\n","        branch7x7 = self.branch7x7_2(branch7x7)\n","        branch7x7 = self.branch7x7_3(branch7x7)\n","\n","        branch7x7dbl = self.branch7x7dbl_1(x)\n","        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n","        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n","        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n","        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n","\n","        # Patch: Tensorflow's average pool does not use the padded zero's in\n","        # its average calculation\n","        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n","                                   count_include_pad=False)\n","        branch_pool = self.branch_pool(branch_pool)\n","\n","        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n","        return torch.cat(outputs, 1)\n","\n","class FIDInceptionE_1(torchvision.models.inception.InceptionE):\n","    '''First InceptionE block patched for FID computation'''\n","    def __init__(self, in_channels):\n","        super(FIDInceptionE_1, self).__init__(in_channels)\n","\n","    def forward(self, x):\n","        branch1x1 = self.branch1x1(x)\n","\n","        branch3x3 = self.branch3x3_1(x)\n","        branch3x3 = [\n","            self.branch3x3_2a(branch3x3),\n","            self.branch3x3_2b(branch3x3),\n","        ]\n","        branch3x3 = torch.cat(branch3x3, 1)\n","\n","        branch3x3dbl = self.branch3x3dbl_1(x)\n","        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n","        branch3x3dbl = [\n","            self.branch3x3dbl_3a(branch3x3dbl),\n","            self.branch3x3dbl_3b(branch3x3dbl),\n","        ]\n","        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n","\n","        # Patch: Tensorflow's average pool does not use the padded zero's in\n","        # its average calculation\n","        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n","                                   count_include_pad=False)\n","        branch_pool = self.branch_pool(branch_pool)\n","\n","        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n","        return torch.cat(outputs, 1)\n","\n","class FIDInceptionE_2(torchvision.models.inception.InceptionE):\n","    '''Second InceptionE block patched for FID computation'''\n","    def __init__(self, in_channels):\n","        super(FIDInceptionE_2, self).__init__(in_channels)\n","\n","    def forward(self, x):\n","        branch1x1 = self.branch1x1(x)\n","\n","        branch3x3 = self.branch3x3_1(x)\n","        branch3x3 = [\n","            self.branch3x3_2a(branch3x3),\n","            self.branch3x3_2b(branch3x3),\n","        ]\n","        branch3x3 = torch.cat(branch3x3, 1)\n","\n","        branch3x3dbl = self.branch3x3dbl_1(x)\n","        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n","        branch3x3dbl = [\n","            self.branch3x3dbl_3a(branch3x3dbl),\n","            self.branch3x3dbl_3b(branch3x3dbl),\n","        ]\n","        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n","\n","        # Patch: The FID Inception model uses max pooling instead of average\n","        # pooling.\n","        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n","        branch_pool = self.branch_pool(branch_pool)\n","\n","        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n","        return torch.cat(outputs, 1)\n","\n","# Define the block index by the feature dimension\n","block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n","# Create the InceptionV3 model instance with the specified block index\n","model = InceptionV3([block_idx])\n","# Move the model to the GPU\n","inception_v3 = model.cuda()"]},{"cell_type":"markdown","metadata":{},"source":["## Fréchet Inception Distance (FID)\n","#### FID measures the distance between two Gaussian distributions approximated from the features of generated and real images. To calculate FID, the mean and covariance of the features extracted from real and generated images are computed. FID is defined as:\n","\n","\\begin{equation}\n","        FID = ||\\mu_r - \\mu_g||^2 + Tr(\\sigma_r + \\sigma_g - 2(\\sigma_r\\sigma_g)^{1/2})\n","\\end{equation}\n","\n","#### where:\n","* #### $\\mu_r$ and $\\mu_g$ are the mean vectors of the features of real and generated images, respectively.\n","* #### $\\sigma_r$ and $\\sigma_g$ are the covariance matrices of the features of real and generated images,   respectively.\n","\n","#### As follows, the implementation of FID for each epoch:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def epoch_calculate_activation_statistics(images, model, batch_size=128, dims=2048, cuda=False):\n","    # Set the model to evaluation mode\n","    model.eval()  \n","    \n","    # Select device\n","    if cuda:\n","        model = model.cuda()  \n","        images = images.cuda()\n","    else:\n","        model = model.cpu()  \n","        images = images.cpu()  \n","\n","    act = np.empty((len(images), dims))\n","    \n","    # No need to track gradients for this operation\n","    with torch.no_grad():  \n","        pred = model(images)\n","        pred = pred[0]\n","        \n","        # Check if the output is 4D (batch, channels, height, width)\n","        if pred.dim() == 4:  \n","            if pred.size(2) != 1 or pred.size(3) != 1:\n","                pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n","            pred = pred.view(pred.size(0), -1)\n","        \n","        # Check if the output is 2D (batch, features)\n","        elif pred.dim() == 2:  \n","            pred = pred\n","        else:\n","            raise RuntimeError(\"Unexpected output dimensions from the model.\")\n","\n","        act = pred.cpu().numpy()\n","\n","    mu = np.mean(act, axis=0)\n","    sigma = np.cov(act, rowvar=False)\n","    return mu, sigma\n","\n","def epoch_calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n","    mu1 = np.atleast_1d(mu1)\n","    mu2 = np.atleast_1d(mu2)\n","\n","    sigma1 = np.atleast_2d(sigma1)\n","    sigma2 = np.atleast_2d(sigma2)\n","\n","    assert mu1.shape == mu2.shape, \\\n","        'Training and test mean vectors have different lengths'\n","    assert sigma1.shape == sigma2.shape, \\\n","        'Training and test covariances have different dimensions'\n","\n","    diff = mu1 - mu2\n","\n","    covmean, _ = sqrtm(sigma1.dot(sigma2), disp=False)\n","    if not np.isfinite(covmean).all():\n","        msg = ('FID calculation produces singular product; '\n","               'adding %s to diagonal of cov estimates') % eps\n","        print(msg)\n","        offset = np.eye(sigma1.shape[0]) * eps\n","        covmean = sqrtm((sigma1 + offset).dot(sigma2 + offset))\n","   \n","    if np.iscomplexobj(covmean):\n","        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n","            m = np.max(np.abs(covmean.imag))\n","            raise ValueError('Imaginary component {}'.format(m))\n","        covmean = covmean.real\n","\n","    tr_covmean = np.trace(covmean)\n","\n","    return (diff.dot(diff) + np.trace(sigma1) +\n","            np.trace(sigma2) - 2 * tr_covmean)\n","\n","\n","def epoch_calculate_fretchet(images_real, images_fake, model):\n","    '''Calculate final value '''\n","    mu_1, std_1 = epoch_calculate_activation_statistics(images_real, model, cuda=True)   \n","    mu_2, std_2 = epoch_calculate_activation_statistics(images_fake, model, cuda=True)\n","    \n","    # Get Frechet distance\n","    fid_value = epoch_calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n","    return fid_value"]},{"cell_type":"markdown","metadata":{},"source":["#### Computing FID on the entire dataset after training : "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# FID from official \"https://github.com/mseitzer/pytorch-fid/blob/master/src/pytorch_fid/fid_score.py\", with a slight modification due to imaginary components of Covariance Matrix \n","  \n","# Supported image formats\n","IMAGE_EXTENSIONS = {\"bmp\", \"jpg\", \"jpeg\", \"pgm\", \"png\", \"ppm\", \"tif\", \"tiff\", \"webp\"}  \n","\n","class ImagePathDataset(torch.utils.data.Dataset):\n","    '''ImagePathDataset is a class for handle the images'''\n","    def __init__(self, files, transforms=None):\n","        self.files = files  # List of image file paths\n","        self.transforms = transforms  # Optional transforms to apply to images\n","\n","    def __len__(self):\n","        '''Return the number of images'''\n","        return len(self.files)  \n","\n","    def __getitem__(self, i):\n","        '''Return the item at position i'''\n","        path = self.files[i]  # Get the image path\n","        img = Image.open(path).convert(\"RGB\")  # Open image and convert to RGB\n","        if self.transforms is not None:\n","            img = self.transforms(img)  # Apply transforms if any\n","        return img  \n","\n","def get_activations(files, model, batch_size=50, dims=2048, device=\"cpu\", num_workers=1):\n","    model.eval()  # Set model to evaluation mode\n","\n","    if batch_size > len(files):\n","        print(\"Warning: batch size is bigger than the data size. Setting batch size to data size\")\n","        batch_size = len(files)  # Adjust batch size if it's larger than the number of files\n","\n","    dataset = ImagePathDataset(files, transforms=TF.ToTensor())  # Create dataset from image files\n","    dataloader = torch.utils.data.DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        drop_last=False,\n","        num_workers=num_workers,\n","    )  # Create dataloader for batching\n","\n","    pred_arr = np.empty((len(files), dims))  # Initialize array to hold activations\n","\n","    start_idx = 0  # Start index for filling pred_arr\n","\n","    for batch in tqdm(dataloader):\n","        batch = batch.to(device)  # Move batch to device\n","\n","        with torch.no_grad():\n","            pred = model(batch)[0]  # Get model predictions\n","\n","        if pred.size(2) != 1 or pred.size(3) != 1:\n","            pred = adaptive_avg_pool2d(pred, output_size=(1, 1))  # Apply adaptive average pooling\n","\n","        pred = pred.squeeze(3).squeeze(2).cpu().numpy()  # Convert predictions to numpy array\n","\n","        pred_arr[start_idx : start_idx + pred.shape[0]] = pred  # Store predictions in pred_arr\n","\n","        start_idx = start_idx + pred.shape[0]  # Update start index\n","\n","    return pred_arr  # Return the array of activations\n","\n","def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n","    '''Compute FID value'''\n","    mu1 = np.atleast_1d(mu1)\n","    mu2 = np.atleast_1d(mu2)\n","\n","    sigma1 = np.atleast_2d(sigma1)\n","    sigma2 = np.atleast_2d(sigma2)\n","\n","    # Ensure that sigma1 and sigma2 have the same shape\n","    max_shape = max(sigma1.shape[0], sigma2.shape[0])\n","    sigma1 = np.pad(sigma1, ((0, max_shape - sigma1.shape[0]), (0, max_shape - sigma1.shape[1])), mode='constant')\n","    sigma2 = np.pad(sigma2, ((0, max_shape - sigma2.shape[0]), (0, max_shape - sigma2.shape[1])), mode='constant')\n","\n","    assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\n","    assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\n","\n","    diff = mu1 - mu2\n","\n","    # Add a small value to the diagonal to improve numerical stability\n","    offset1 = np.eye(sigma1.shape[0]) * eps\n","    offset2 = np.eye(sigma2.shape[0]) * eps\n","\n","    covmean, _ = linalg.sqrtm((sigma1 + offset1).dot(sigma2 + offset2), disp=False)\n","    if not np.isfinite(covmean).all():\n","        msg = \"fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps\n","        print(msg)\n","        covmean = linalg.sqrtm((sigma1 + offset1).dot(sigma2 + offset2))\n","\n","    if np.iscomplexobj(covmean):\n","        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n","            m = np.max(np.abs(covmean.imag))\n","            raise ValueError(\"Imaginary component {}\".format(m))\n","        covmean = covmean.real\n","\n","    tr_covmean = np.trace(covmean)\n","\n","    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n","\n","def calculate_activation_statistics(files, model, batch_size=50, dims=2048, device=\"cpu\", num_workers=1):\n","    act = get_activations(files, model, batch_size, dims, device, num_workers)  # Get activations for the files\n","    mu = np.mean(act, axis=0)  # Calculate mean of activations\n","    sigma = np.cov(act, rowvar=False)  # Calculate covariance of activations\n","    return mu, sigma  \n","\n","def compute_statistics_of_path(path, model, batch_size, dims, device, num_workers=1):\n","    if path.endswith(\".npz\"):\n","        with np.load(path) as f:\n","            m, s = f[\"mu\"][:], f[\"sigma\"][:]  # Load precomputed statistics\n","    else:\n","        path = pathlib.Path(path)\n","        files = sorted([file for ext in IMAGE_EXTENSIONS for file in path.glob(\"*.{}\".format(ext))])  # Gather image files\n","        m, s = calculate_activation_statistics(files, model, batch_size, dims, device, num_workers)  # Compute statistics\n","\n","    return m, s  # Return mean and covariance\n","\n","def calculate_fid_given_paths(paths, batch_size, device, dims, num_workers=1):\n","    for p in paths:\n","        if not os.path.exists(p):\n","            raise RuntimeError(\"Invalid path: %s\" % p)  # Check if paths exist\n","\n","    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]  # Get block index for InceptionV3\n","\n","    model = InceptionV3([block_idx]).to(device)  # Initialize InceptionV3 model\n","\n","    m1, s1 = compute_statistics_of_path(paths[0], model, batch_size, dims, device, num_workers)  # Compute statistics for first path\n","    m2, s2 = compute_statistics_of_path(paths[1], model, batch_size, dims, device, num_workers)  # Compute statistics for second path\n","    fid_value = calculate_frechet_distance(m1, s1, m2, s2)  # Calculate FID\n","\n","    return fid_value  # Return FID value\n","\n","def save_fid_stats(paths, batch_size, device, dims, num_workers=1):\n","    if not os.path.exists(paths[0]):\n","        raise RuntimeError(\"Invalid path: %s\" % paths[0])  # Check if input path exists\n","\n","    if os.path.exists(paths[1]):\n","        raise RuntimeError(\"Existing output file: %s\" % paths[1])  # Check if output file already exists\n","\n","    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]  # Get block index for InceptionV3\n","\n","    model = InceptionV3([block_idx]).to(device)  # Initialize InceptionV3 model\n","\n","    print(f\"Saving statistics for {paths[0]}\")\n","\n","    m1, s1 = compute_statistics_of_path(paths[0], model, batch_size, dims, device, num_workers)  # Compute statistics for input path\n","\n","    np.savez_compressed(paths[1], mu=m1, sigma=s1)  # Save statistics to file"]},{"cell_type":"markdown","metadata":{},"source":["## Kernel Inception Distance (KID)\n","\n","#### KID measures the distance between the distributions of features from generated images and real images using the Maximum Mean Discrepancy (MMD) with a specific kernel, typically the Gaussian kernel.\n","\n","#### MMD is a metric that evaluates how different two sets of samples (in this case, the features of the images) are. \n","\n","#### As follows, the implementation of KID for each epoch:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def epoch_polynomial_mmd(codes_g, codes_r, degree=3, gamma=None, coef0=1, var_at_m=None, ret_var=True):\n","    '''Compute the polynomial MMD (Maximum Mean Discrepancy)'''\n","    # Compute the polynomial kernel for generated vs generated, real vs real, and generated vs real\n","    K_XX = polynomial_kernel(codes_g, codes_g, degree=degree, gamma=gamma, coef0=coef0)\n","    K_YY = polynomial_kernel(codes_r, codes_r, degree=degree, gamma=gamma, coef0=coef0)\n","    K_XY = polynomial_kernel(codes_g, codes_r, degree=degree, gamma=gamma, coef0=coef0)\n","\n","    # Calculate the MMD value\n","    m = K_XX.shape[0]\n","    mmd2 = (np.mean(K_XX) + np.mean(K_YY) - 2 * np.mean(K_XY))\n","\n","    if not ret_var:\n","        return mmd2\n","\n","    return mmd2, None\n","\n","def epoch_polynomial_mmd_averages(codes_g, codes_r, n_subsets=10, subset_size=1000, ret_var=True, output=sys.stdout, **kernel_args):\n","    '''Compute the polynomial MMD averages over multiple subsets'''\n","    # Adjust subset size if it's larger than the number of available codes\n","    actual_subset_size = min(subset_size, len(codes_g), len(codes_r))\n","\n","    m = min(len(codes_g), len(codes_r))\n","    mmds = np.zeros(n_subsets)\n","    vars = np.zeros(n_subsets) if ret_var else None\n","    choice = np.random.choice\n","\n","    # Iterate over the number of subsets and compute MMD for each subset\n","    with tqdm(range(n_subsets), desc='MMD', file=output) as bar:\n","        for i in bar:\n","            if actual_subset_size < subset_size:\n","                # If the actual subset size is smaller than desired, allow replacement\n","                g = codes_g[choice(len(codes_g), actual_subset_size, replace=True)]\n","                r = codes_r[choice(len(codes_r), actual_subset_size, replace=True)]\n","            else:\n","                g = codes_g[choice(len(codes_g), actual_subset_size, replace=False)]\n","                r = codes_r[choice(len(codes_r), actual_subset_size, replace=False)]\n","            o = epoch_polynomial_mmd(g, r, **kernel_args, var_at_m=m, ret_var=ret_var)\n","            if ret_var:\n","                mmds[i], vars[i] = o\n","            else:\n","                mmds[i] = o\n","            bar.set_postfix({'mean': mmds[:i+1].mean()})\n","    return (mmds, vars) if ret_var else mmds\n","\n","def epoch_calculate_kid_given_activations(activations_real, activations_fake):\n","    '''Compute KID (Kernel Inception Distance) given activations'''\n","    return epoch_polynomial_mmd_averages(activations_real, activations_fake, n_subsets=10)\n","\n","def epoch_calculate_activations(images, model, cuda=False):\n","    '''Compute activations of images using the model'''\n","    model.eval()\n","    batch_size = images.size(0)\n","    if cuda:\n","        images = images.cuda()\n","        model.cuda()\n","    with torch.no_grad():\n","        pred = model(images)\n","        pred = pred[0]\n","        # Check if the output is 4D (batch, channels, height, width)\n","        if pred.dim() == 4:  \n","            pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n","        pred = pred.view(batch_size, -1)\n","    # Return as numpy array for consistency with KID function\n","    return pred.cpu().numpy()  \n","\n","def epoch_compute_mmd_simple(codes_g, codes_r, degree=3, gamma=None, coef0=1):\n","    '''Compute MMD (Maximum Mean Discrepancy) using a simple polynomial kernel'''\n","    if gamma is None:\n","        # Default gamma is 1/number of features\n","        gamma = 1.0 / codes_g.shape[1]  \n","\n","    # Compute the polynomial kernel for generated vs generated, real vs real, and generated vs real\n","    K_XX = polynomial_kernel(codes_g, codes_g, degree=degree, gamma=gamma, coef0=coef0)\n","    K_YY = polynomial_kernel(codes_r, codes_r, degree=degree, gamma=gamma, coef0=coef0)\n","    K_XY = polynomial_kernel(codes_g, codes_r, degree=degree, gamma=gamma, coef0=coef0)\n","\n","    # Calculate the MMD value\n","    mmd2 = np.mean(K_XX) + np.mean(K_YY) - 2 * np.mean(K_XY)\n","    return mmd2"]},{"cell_type":"markdown","metadata":{},"source":["#### Computing KID on the entire dataset after training : "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\" Computing KID on the entire dataset \"\"\"\n","\n","def get_activations(files, model, batch_size=1, dims=2048,\n","                    cuda=False, verbose=False):\n","    \"\"\"Calculates the activations of the pool_3 layer for all images.\"\"\"\n","    # Set the model to evaluation mode\n","    model.eval()\n","    # Determine if the input is numpy arrays\n","    is_numpy = True if type(files[0]) == np.ndarray else False\n","\n","    # Check if the number of images is a multiple of the batch size\n","    if len(files) % batch_size != 0:\n","        print(('Warning: number of images is not a multiple of the '\n","               'batch size. Some samples are going to be ignored.'))\n","    # Adjust batch size if it is larger than the number of images\n","    if batch_size > len(files):\n","        print(('Warning: batch size is bigger than the data size. '\n","               'Setting batch size to data size'))\n","        batch_size = len(files)\n","\n","    # Calculate the number of batches and the number of images used\n","    n_batches = len(files) // batch_size\n","    n_used_imgs = n_batches * batch_size\n","\n","    # Initialize an array to store the activations\n","    pred_arr = np.empty((n_used_imgs, dims))\n","\n","    # Loop through each batch\n","    for i in tqdm(range(n_batches)):\n","        if verbose:\n","            print('\\rPropagating batch %d/%d' % (i + 1, n_batches), end='', flush=True)\n","        start = i * batch_size\n","        end = start + batch_size\n","\n","        # Preprocess images based on their format\n","        if is_numpy:\n","            images = np.copy(files[start:end]) + 1\n","            images /= 2.\n","        else:\n","            images = [np.array(Image.open(str(f))) for f in files[start:end]]\n","            images = np.stack(images).astype(np.float32) / 255.\n","            images = torch.from_numpy(images)\n","            if len(images.shape) == 3:\n","                images = torch.unsqueeze(images, dim=-1).expand(-1, -1, -1, 3)\n","            images = images.permute((0, 3, 1, 2))\n","\n","        batch = images.float()\n","        if cuda:\n","            batch = batch.cuda()\n","\n","        # Get the model predictions\n","        pred = model(batch)[0]\n","\n","        # Apply adaptive average pooling if the output dimensions are not 1x1\n","        if pred.shape[2] != 1 or pred.shape[3] != 1:\n","            pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n","\n","        # Store the activations in the array\n","        pred_arr[start:end] = pred.cpu().data.numpy().reshape(batch_size, -1)\n","\n","    if verbose:\n","        print('done', torch.min(images))\n","\n","    return pred_arr\n","\n","def extract_lenet_features(imgs, net):\n","    \"\"\"Extract features using a LeNet model.\"\"\"\n","    net.eval()\n","    feats = []\n","    imgs = imgs.reshape([-1, 100] + list(imgs.shape[1:]))\n","    if imgs[0].min() < -0.001:\n","        imgs = (imgs + 1) / 2.0\n","    print(imgs.shape, imgs.min(), imgs.max())\n","    imgs = torch.from_numpy(imgs)\n","    for i, images in enumerate(imgs):\n","        feats.append(net.extract_features(images).detach().cpu().numpy())\n","    feats = np.vstack(feats)\n","    return feats\n","\n","def _compute_activations(path, model, batch_size, dims, cuda, model_type):\n","    \"\"\"Compute activations for the given path using the specified model.\"\"\"\n","    if not type(path) == np.ndarray:\n","        import glob\n","        jpg = os.path.join(path, '*.jpg')\n","        png = os.path.join(path, '*.png')\n","        path = glob.glob(jpg) + glob.glob(png)\n","        if len(path) > 50000:\n","            import random\n","            random.shuffle(path)\n","            path = path[:50000]\n","    if model_type == 'inception':\n","        act = get_activations(path, model, batch_size, dims, cuda)\n","    elif model_type == 'lenet':\n","        act = extract_lenet_features(path, model)\n","    return act\n","\n","def calculate_kid_given_paths(paths, batch_size, cuda, dims, model_type='inception'):\n","    \"\"\"Calculates the KID of two paths\"\"\"\n","    pths = []\n","    for p in paths:\n","        if not os.path.exists(p):\n","            raise RuntimeError('Invalid path: %s' % p)\n","        if os.path.isdir(p):\n","            pths.append(p)\n","        elif p.endswith('.npy'):\n","            np_imgs = np.load(p)\n","            if np_imgs.shape[0] > 50000: np_imgs = np_imgs[np.random.permutation(np.arange(np_imgs.shape[0]))][:50000]\n","            pths.append(np_imgs)\n","\n","    if model_type == 'inception':\n","        block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n","        model = InceptionV3([block_idx])\n","    elif model_type == 'lenet':\n","        model = LeNet5()\n","        model.load_state_dict(torch.load('./models/lenet.pth'))\n","    if cuda:\n","        model.cuda()\n","\n","    act_true = _compute_activations(pths[0], model, batch_size, dims, cuda, model_type)\n","    pths = pths[1:]\n","    results = []\n","    for j, pth in enumerate(pths):\n","        print(paths[j + 1])\n","        actj = _compute_activations(pth, model, batch_size, dims, cuda, model_type)\n","        kid_values = polynomial_mmd_averages(act_true, actj, n_subsets=100)\n","        results.append((paths[j + 1], kid_values[0].mean(), kid_values[0].std()))\n","    return results\n","\n","def _sqn(arr):\n","    \"\"\"Square norm of the flattened array.\"\"\"\n","    flat = np.ravel(arr)\n","    return flat.dot(flat)\n","\n","def polynomial_mmd_averages(codes_g, codes_r, n_subsets=50, subset_size=1000,\n","                            ret_var=True, output=sys.stdout, **kernel_args):\n","    \"\"\"Compute MMD averages over several subsets.\"\"\"\n","    m = min(codes_g.shape[0], codes_r.shape[0])\n","    subset_size = min(subset_size, m)  # Ensure subset_size is not larger than available samples\n","    mmds = np.zeros(n_subsets)\n","    if ret_var:\n","        vars = np.zeros(n_subsets)\n","    choice = np.random.choice\n","\n","    with tqdm(range(n_subsets), desc='MMD', file=output) as bar:\n","        for i in bar:\n","            g = codes_g[choice(len(codes_g), subset_size, replace=False)]\n","            r = codes_r[choice(len(codes_r), subset_size, replace=False)]\n","            o = polynomial_mmd(g, r, **kernel_args, var_at_m=m, ret_var=ret_var)\n","            if ret_var:\n","                mmds[i], vars[i] = o\n","            else:\n","                mmds[i] = o\n","            bar.set_postfix({'mean': mmds[:i+1].mean()})\n","    return (mmds, vars) if ret_var else mmds\n","\n","def polynomial_mmd(codes_g, codes_r, degree=3, gamma=None, coef0=1,\n","                   var_at_m=None, ret_var=True):\n","    \"\"\"Compute polynomial MMD between two sets of codes.\"\"\"\n","    X = codes_g\n","    Y = codes_r\n","\n","    K_XX = polynomial_kernel(X, degree=degree, gamma=gamma, coef0=coef0)\n","    K_YY = polynomial_kernel(Y, degree=degree, gamma=gamma, coef0=coef0)\n","    K_XY = polynomial_kernel(X, Y, degree=degree, gamma=gamma, coef0=coef0)\n","\n","    return _mmd2_and_variance(K_XX, K_XY, K_YY,\n","                              var_at_m=var_at_m, ret_var=ret_var)\n","\n","def _mmd2_and_variance(K_XX, K_XY, K_YY, unit_diagonal=False,\n","                       mmd_est='unbiased', block_size=1024,\n","                       var_at_m=None, ret_var=True):\n","    \"\"\"Calculate the MMD2 (Maximum Mean Discrepancy) and its variance.\"\"\"\n","    \n","    m = K_XX.shape[0]  # Number of samples\n","\n","    # Ensure that the kernel matrices have the correct shapes\n","    assert K_XX.shape == (m, m)\n","    assert K_XY.shape == (m, m)\n","    assert K_YY.shape == (m, m)\n","\n","    # If var_at_m is not provided, set it to m\n","    if var_at_m is None:\n","        var_at_m = m\n","\n","    # Initialize diagonal and sum variables based on whether unit diagonal is used\n","    if unit_diagonal:\n","        diag_X = diag_Y = 1\n","        sum_diag_X = sum_diag_Y = m\n","        sum_diag2_X = sum_diag2_Y = m\n","    else:\n","        diag_X = np.diagonal(K_XX)\n","        diag_Y = np.diagonal(K_YY)\n","        sum_diag_X = diag_X.sum()\n","        sum_diag_Y = diag_Y.sum()\n","        sum_diag2_X = _sqn(diag_X)\n","        sum_diag2_Y = _sqn(diag_Y)\n","\n","    # Calculate the sum of the kernel matrices excluding the diagonal\n","    Kt_XX_sums = K_XX.sum(axis=1) - diag_X\n","    Kt_YY_sums = K_YY.sum(axis=1) - diag_Y\n","    K_XY_sums_0 = K_XY.sum(axis=0)\n","    K_XY_sums_1 = K_XY.sum(axis=1)\n","\n","    Kt_XX_sum = Kt_XX_sums.sum()\n","    Kt_YY_sum = Kt_YY_sums.sum()\n","    K_XY_sum = K_XY_sums_0.sum()\n","\n","    # Calculate MMD2 based on the chosen estimation method\n","    if mmd_est == 'biased':\n","        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)\n","                + (Kt_YY_sum + sum_diag_Y) / (m * m)\n","                - 2 * K_XY_sum / (m * m))\n","    else:\n","        assert mmd_est in {'unbiased', 'u-statistic'}\n","        mmd2 = (Kt_XX_sum + Kt_YY_sum) / (m * (m - 1))\n","        if mmd_est == 'unbiased':\n","            mmd2 -= 2 * K_XY_sum / (m * m)\n","        else:\n","            mmd2 -= 2 * (K_XY_sum - np.trace(K_XY)) / (m * (m - 1))\n","\n","    # Return MMD2 if variance is not required\n","    if not ret_var:\n","        return mmd2\n","\n","    # Calculate terms needed for variance estimation\n","    Kt_XX_2_sum = _sqn(K_XX) - sum_diag2_X\n","    Kt_YY_2_sum = _sqn(K_YY) - sum_diag2_Y\n","    K_XY_2_sum = _sqn(K_XY)\n","    dot_XX_XY = Kt_XX_sums.dot(K_XY_sums_1)\n","    dot_YY_YX = Kt_YY_sums.dot(K_XY_sums_0)\n","    \n","    if m <= 2:\n","        # Return zero variance if m is less than or equal to 2\n","        return mmd2, 0\n","    \n","    m1 = m - 1\n","    m2 = m - 2\n","\n","    # Estimate zeta1 and zeta2 for variance calculation\n","    zeta1_est = (\n","        1 / (m * m1 * m2) * (\n","            _sqn(Kt_XX_sums) - Kt_XX_2_sum + _sqn(Kt_YY_sums) - Kt_YY_2_sum)\n","        - 1 / (m * m1) ** 2 * (Kt_XX_sum ** 2 + Kt_YY_sum ** 2)\n","        + 1 / (m * m * m1) * (\n","            _sqn(K_XY_sums_1) + _sqn(K_XY_sums_0) - 2 * K_XY_2_sum)\n","        - 2 / m ** 4 * K_XY_sum ** 2\n","        - 2 / (m * m * m1) * (dot_XX_XY + dot_YY_YX)\n","        + 2 / (m ** 3 * m1) * (Kt_XX_sum + Kt_YY_sum) * K_XY_sum\n","    )\n","    zeta2_est = (\n","        1 / (m * m1) * (Kt_XX_2_sum + Kt_YY_2_sum)\n","        - 1 / (m * m1) ** 2 * (Kt_XX_sum ** 2 + Kt_YY_sum ** 2)\n","        + 2 / (m * m) * K_XY_2_sum\n","        - 2 / m ** 4 * K_XY_sum ** 2\n","        - 4 / (m * m * m1) * (dot_XX_XY + dot_YY_YX)\n","        + 4 / (m ** 3 * m1) * (Kt_XX_sum + Kt_YY_sum) * K_XY_sum\n","    )\n","    var_est = (4 * (var_at_m - 2) / (var_at_m * (var_at_m - 1)) * zeta1_est\n","               + 2 / (var_at_m * (var_at_m - 1)) * zeta2_est)\n","\n","    return mmd2, var_est  # Return MMD2 and its variance"]},{"cell_type":"markdown","metadata":{},"source":["## Training SB Model \n","\n","#### The training loop is defined as follows : "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if __name__ == '__main__':\n","\n","    # Model \n","    sb_model_compl = SBModel().to(device)\n","    \n","    total_iters = 0      \n","    optimize_time = 0.1\n","    epoch_count = 1\n","    n_epochs = 90\n","    n_epochs_decay = 90\n","    print_freq = 100 \n","    gpu_ids = [1]\n","    output_dir = \"/kaggle/working/generated_images\"\n","    \n","    # Lists for Losses, FID and KID metrics \n","    losses_list = []\n","    fid_list = []\n","    kid_list = []\n","    \n","    sb_model_compl.train()\n","    \n","    # Training \n","    times = []\n","    for epoch in range(epoch_count, n_epochs + n_epochs_decay +1):    \n","        epoch_start_time = time.time()  \n","        iter_data_time = time.time()    \n","        epoch_iter = 0     # the number of training iterations in current epoch, reset to 0 every epoch\n","         \n","        \n","        for i, ((dataA, dataB), (dataA2, dataB2)) in enumerate(zip(zip(train_dataloaderA, train_dataloaderB), zip(train_dataloaderA, train_dataloaderB))):  \n","            dataA = dataA.to(device)\n","            dataB = dataB.to(device)\n","            dataA2 = dataA2.to(device)\n","            dataB2 = dataB2.to(device)\n","            \n","            iter_start_time = time.time()  \n","            if total_iters % print_freq == 0:\n","                t_data = iter_start_time - iter_data_time\n","\n","            batch_size = 1\n","            total_iters += batch_size\n","            epoch_iter += batch_size\n","            if len(gpu_ids) > 0:\n","                torch.cuda.synchronize()\n","            optimize_start_time = time.time()\n","            if epoch == epoch_count and i == 0:\n","                sb_model_compl.data_dependent_initialize(dataA,dataB, dataA2, dataB2)\n","            sb_model_compl.set_input(dataA,dataB, dataA2, dataB2)  # unpack data from dataset and apply preprocessing\n","            sb_model_compl.optimize_parameters()   # calculate loss functions, get gradients, update network weights\n","            \n","            torch.cuda.synchronize()\n","            optimize_time = (time.time() - optimize_start_time) / batch_size * 0.005 + 0.995 * optimize_time\n","            \n","            if total_iters % print_freq == 0:    # print training losses and save logging information \n","                losses = sb_model_compl.get_current_losses()\n","                print(losses)\n","                \n","                \n","\n","            iter_data_time = time.time()\n","\n","        \n","        print('End of epoch %d / %d \\t Time Taken: %d sec' % (epoch, n_epochs + n_epochs_decay, time.time() - epoch_start_time))\n","         \n","        # Visualize and save the generated fake images at the end of each epoch\n","        sb_model_compl.forward()  \n","        fake_B_images = sb_model_compl.fake_B\n","        fake_B2_images = sb_model_compl.fake_B2\n","        real_A_images = sb_model_compl.real_A\n","        real_B_images = sb_model_compl.real_B\n","        visualize_images(fake_B_images, title=f\"Generated Zebras at Epoch {epoch}\")\n","        visualize_images(fake_B2_images, title=f\"Generated Zebras 2 at Epoch {epoch}\")\n","        \n","        # Save the generated images\n","        save_image(fake_B_images, os.path.join(generated_images, f\"generated_zebras_epoch{epoch}.png\"))\n","        save_image(fake_B2_images, os.path.join(generated_images, f\"generated_zebras_2_epoch{epoch}.png\"))\n","        \n","        # Compute FID \n","        fretchet_dist= epoch_calculate_fretchet(dataB,sb_model_compl.fake_B.to(device),inception_v3)\n","        print(f'Epoch {epoch}: FID:', fretchet_dist)\n","        \n","        # Compute activations for KID per epoch \n","        activations_real = epoch_calculate_activations(dataB, inception_v3, cuda=True)\n","        activations_fake = epoch_calculate_activations(sb_model_compl.fake_B.to(device), inception_v3, cuda=True)\n","\n","        # Calculate KID\n","        kid_value = epoch_compute_mmd_simple(activations_real, activations_fake)\n","        print(f'Epoch {epoch}: KID: {kid_value}')\n","        \n","        losses_list.append(losses)\n","        fid_list.append(fretchet_dist)\n","        kid_list.append(kid_value)"]},{"cell_type":"markdown","metadata":{},"source":["#### Computing FID and KID on the entire dataset after training : "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["DIMS = 2048\n","PATHS = [\"/kaggle/input/horse2zebra-new/horse2zebra/small_trainB\", \"/kaggle/working/generated_images\"]  \n","NUM_WORKERS = 1\n","MODEL_TYPE = 'inception'\n","\n","fid_value = calculate_fid_given_paths(PATHS, BATCH_SIZE, device, DIMS, NUM_WORKERS)\n","print(\"FID: \", fid_value)\n","\n","results = calculate_kid_given_paths(PATHS, BATCH_SIZE, device, DIMS, model_type= MODEL_TYPE)\n","for p, m, s in results:\n","    print('KID (%s): %.3f (%.3f)' % (p, m, s))"]},{"cell_type":"markdown","metadata":{},"source":["#### Plotting losses, KID and FID for training: "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define the range of epochs based on the length of losses_list\n","epochs = range(1, len(losses_list) + 1)\n","\n","# Initialize dictionaries to store the metrics across epochs\n","# Initialize lists based on keys in the first dictionary of losses_list\n","metrics = {key: [] for key in losses_list[0]} \n","\n","# Populate the lists with values for each epoch\n","for loss_dict in losses_list:\n","    for key, value in loss_dict.items():\n","        metrics[key].append(value)\n","\n","# Create a plot figure\n","plt.figure(figsize=(12, 8))\n","\n","# Plot each metric stored in the metrics dictionary\n","for metric, values in metrics.items():\n","    plt.plot(values, label=metric)\n","\n","plt.title('Training Loss Metrics over Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss Value')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# Plot FID\n","plt.figure(figsize=(10, 5))\n","plt.plot(epochs, fid_list, label='FID')\n","plt.xlabel('Epochs')\n","plt.ylabel('FID Value')\n","plt.title('Fréchet Inception Distance (FID)')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# Plot KID\n","plt.figure(figsize=(10, 5))\n","plt.plot(epochs, kid_list, label='KID')\n","plt.xlabel('Epochs')\n","plt.ylabel('KID Value')\n","plt.title('Kernel Inception Distance (KID)')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### Saving the model : "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# saving the model\n","torch.save(sb_model_compl.state_dict(), \"/kaggle/working/our_pretrained_sb_model.pth\")"]},{"cell_type":"markdown","metadata":{},"source":["## SB Test \n","\n","#### For the testing phase, we defined the same model, adapting the functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class SBModel_test(nn.Module):\n","    \"\"\" Initializes the SBModel class, setting up parameters, loss names, model names, visual names, optimizers, and other necessary configurations \"\"\"\n","    def __init__(self):\n","        super(SBModel_test,self).__init__()\n","        self.visual_names = ['real']\n","        self.T = 5\n","        for NFE in range(self.T):\n","                fake_name = 'fake_' + str(NFE+1)\n","                self.visual_names.append(fake_name)\n","                \n","        self.tau = 0.1 \n","        self.device = device\n","        self.netG = gen\n","        self.ngf = 64\n","        self.lr = 0.00001\n","        self.beta1 = 0.5\n","        self.beta2 = 0.999\n","        \n","        \n","    def data_dependent_initialize(self, dataA,dataB): \n","        \"\"\" Initializes the model using input data. It sets the input, trims the batch size, and performs a forward pass \n","        to prepare the model for evaluation \"\"\"\n","        \n","        bs = 1\n","        self.set_input(dataA,dataB)\n","        self.real_A = self.real_A[:bs]\n","        self.real_B = self.real_B[:bs]\n","        self.forward()   \n","    \n","    def set_input(self, dataA, dataB):\n","        \"\"\" Responsible for unpacking input data from the dataloader and performing any necessary preprocessing steps \"\"\"\n","        self.real_A = dataA.to(device)\n","        self.real_B = dataB.to(device)\n","    \n","    \n","    def forward(self):\n","        \"\"\" It processes the input tensor through a series of timesteps, refining it iteratively by \n","        blending it with noise and the generator's output at each step. The time steps are predefined and normalized, \n","        and a random time index is selected to determine which time step to use for each batch element. The generator \n","        is used in evaluation mode to produce outputs based on the current input tensor, the time step, and some random noise. \"\"\"\n","        tau = 0.01\n","        T = 5\n","        incs = np.array([0] + [1/(i+1) for i in range(T-1)])\n","        times = np.cumsum(incs)\n","        times = times / times[-1]\n","        times = 0.5 * times[-1] + 0.5 * times\n","        times = torch.tensor(times).float().cuda()\n","        self.times = times\n","        bs =  1\n","        time_idx = (torch.randint(T, size=[1]).cuda() * torch.ones(size=[bs]).cuda()).long()\n","        self.time_idx = time_idx\n","        self.timestep     = times[time_idx]\n","        with torch.no_grad():\n","            self.netG.eval()\n","            for t in range(T):\n","                if t > 0:\n","                    delta = times[t] - times[t-1]\n","                    denom = times[-1] - times[t-1]\n","                    inter = (delta / denom)\n","                    scale = (delta * (1 - delta / denom))\n","                Xt       = self.real_A if (t == 0) else (1-inter) * Xt + inter * Xt_1.detach() + (scale * tau).sqrt() * torch.randn_like(Xt).to(self.real_A.device)\n","                time_idx = (t * torch.ones(size=[self.real_A.shape[0]]).to(self.real_A.device)).long()\n","                time     = times[time_idx]\n","                z        = torch.randn(size=[self.real_A.shape[0],4*self.ngf]).to(self.real_A.device)\n","                Xt_1     = self.netG(Xt, time_idx, z)\n","\n","                self.Xt_1 = Xt_1"]},{"cell_type":"markdown","metadata":{},"source":["\n","#### Testing the model : "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create output images directories\n","results_dir = '/kaggle/working/results_dir'\n","if not os.path.exists(results_dir):\n","    os.makedirs('/kaggle/working/results_dir')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if __name__ == '__main__':\n","    # Initialize test parameters\n","    aspect_ratio = 1.0\n","    \n","    # Hard-code some parameters for the test\n","    num_threads = 0   # Test code only supports num_threads = 1\n","    batch_size = 1    # Test code only supports batch_size = 1\n","    serial_batches = True  # Disable data shuffling\n","    no_flip = True    # No flip\n","    \n","    sb_model_test = SBModel_test().to(device)\n","    \n","    pretrained_dict = torch.load(\"/kaggle/working/our_pretrained_sb_model.pth\")\n","    model_dict = sb_model_test.state_dict()\n","    \n","    fid_list_test = []\n","    kid_list_test = []\n","\n","    # Filter out unnecessary keys\n","    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and model_dict[k].size() == v.size()}\n","    # Update the new model's dict with the pretrained dict\n","    model_dict.update(pretrained_dict)  \n","    sb_model_test.load_state_dict(model_dict)\n","    sb_model_test.eval()\n","    \n","    for i, (dataA, dataB) in enumerate(zip(test_dataloaderA, test_dataloaderB)):\n","        dataA = dataA.to(device)\n","        dataB = dataB.to(device)\n","        if i == 0:\n","            sb_model_test.data_dependent_initialize(dataA, dataB)\n","            sb_model_test.eval()\n","            \n","        # Unpack data from data loader\n","        sb_model_test.set_input(dataA, dataB)  \n","        # Run inference\n","        sb_model_test.forward()  \n","        \n","        fake_B_images = sb_model_test.Xt_1\n","        visualize_images(fake_B_images.to(device), title=\"Generated Zebras\")\n","        \n","        # Save the generated images if needed\n","        save_image(fake_B_images, os.path.join(results_dir, f\"generated_zebras_{i}.png\"))\n","        \n","        #Compute FID \n","        fretchet_dist = epoch_calculate_fretchet(dataB, fake_B_images.to(device), inception_v3)\n","        print('FID:', fretchet_dist)\n","        \n","        # Compute activations\n","        activations_real = epoch_calculate_activations(dataB, inception_v3, cuda=True)\n","        activations_fake = epoch_calculate_activations(fake_B_images.to(device), inception_v3, cuda=True)\n","        \n","        # Calculate KID\n","        kid_value = epoch_compute_mmd_simple(activations_real, activations_fake)\n","        print('KID:', kid_value)\n","        \n","        fid_list_test.append(fretchet_dist)\n","        kid_list_test.append(kid_value)\n","        "]},{"cell_type":"markdown","metadata":{},"source":["#### Plotting FID and KID for testing:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plotting FID and KID\n","plt.figure(figsize=(12, 6))\n","\n","# Plot FID\n","plt.subplot(1, 2, 1)\n","plt.plot(fid_list_test, label='FID')\n","plt.xlabel('Test Sample')\n","plt.ylabel('FID Score')\n","plt.title('FID over Test Samples')\n","plt.legend()\n","\n","# Plot KID\n","plt.subplot(1, 2, 2)\n","plt.plot(kid_list_test, label='KID')\n","plt.xlabel('Test Sample')\n","plt.ylabel('KID Score')\n","plt.title('KID over Test Samples')\n","plt.legend()\n","\n","# Show plots\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### To conclude, we calculate the FID and KID values on the entire dataset after the testing phase "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define parameters here\n","PATHS = [\"/kaggle/input/horse2zebra-new/horse2zebra/testB\", \"/kaggle/working/results_dir\"]  # Replace with your actual paths\n","\n","fid_value = calculate_fid_given_paths(PATHS, BATCH_SIZE, device, DIMS, NUM_WORKERS)\n","print(\"FID Test: \", fid_value)\n","\n","kid_values = calculate_kid_given_paths(PATHS, BATCH_SIZE, device, DIMS, model_type = MODEL_TYPE)\n","for p, m, s in kid_values:\n","    print('KID Test (%s): %.3f (%.3f)' % (p, m, s))"]},{"cell_type":"markdown","metadata":{},"source":["## Extras : Further Experiments "]},{"cell_type":"markdown","metadata":{},"source":["### We conducted several experiments to determine which of the various implemented architectures produced the best results (see **experiments** folder in our **github repository** for major details)"]},{"cell_type":"markdown","metadata":{},"source":["### 1. Feature Injection"]},{"cell_type":"markdown","metadata":{},"source":["#### During the re-implementation of the model, we tried to develop a generator that used Feature Injection.\n","#### This technique aims to enhance the network's flexibility for applications like image generation or style transfer by conditioning the transformations within the network on additional, contextually relevant data.\n","#### This process dynamically modifies the network’s behavior, which theoretically allows for richer and more context-sensitive outputs. \n","#### However, a major issue was encountered with this approach, as indicated by unusually high loss values during training (G_GAN Loss: 3.2). This could be a result of the added complexity and potential instabilities introduced by direct feature modification, which might not be well-controlled by the current training regimen or data normalization strategies.\n","#### Visually, the generated images were unrealistic, as:"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"text-align:center\">\n","    <img src=\"images/feature_injection_1.png\" alt=\"Immagine 1\" width=\"300\" style=\"margin-right: 20px\">\n","    <img src=\"images/feature_injection_2.png\" alt=\"Immagine 2\" width=\"300\" style=\"margin-left: 20px\">\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["### 2. CycleGAN Training \n","#### Furthermore, before creating the final diffusion model, we tested our definitive generator and discriminator using CycleGAN training to better understand their functionality. Below are some images obtained  "]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"display: flex; justify-content: center; align-items: center;\">\n","  <figure style=\"margin: 0 20px; text-align: center;\">\n","    <img src=\"images/cyclegan_1.png\" width=\"300\">\n","    <figcaption><strong>From horse to zebra</strong></figcaption>\n","  </figure>\n","  <figure style=\"margin: 0 20px; text-align: center;\">\n","    <img src=\"images/cyclegan_2.png\" width=\"300\">\n","    <figcaption><strong>From zebra to horse</strong></figcaption>\n","  </figure>\n","</div>\n"]},{"cell_type":"markdown","metadata":{},"source":["### 3. Epochs and Reduced Dataset \n","\n","#### Last but not least, we want to mention that we are not training our model on the full horse2zebra dataset. As specified at the beginning, the model was trained on \"Kaggle\", using GPU P100. \n","#### Due to limited computational resources, we opted to train the model on a smaller subset of the original dataset. Specifically, we run several experiments using  \n","\n","- #### **Training Datasets**: at the beginning, 200 images for each of the horse and zebra categories in the training set. Then, we tested with 400 images for both trainA annd trainB, totaling 800 images for training \n","- #### **Testing Datasets**: for testing, each category—horse and zebra—includes 120 images. \n","\n","#### We observed that increasing the number of images in the dataset also increases the computational time per epoch. With 200 images for trainA and trainB, each epoch took approximately 220 seconds. Instead, with 400 images for trainA and trainB, each epoch took about 440 seconds. Since we have only 12 hours available to run a notebook on Kaggle, we decided to limit the number of epochs accordingly.\n","\n","#### Although a larger dataset allows for faster convergence, we achieved good results despite running fewer epochs than the 400 epochs used in the original paper, which utilized the entire dataset.\n","\n","#### Following, we show the obtained results: "]},{"cell_type":"markdown","metadata":{},"source":["- #### Training with 200 images for trainA and trainB for 180 epochs   "]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"text-align:center\">\n","    <img src=\"images/zebra1_results.png\" alt=\"Zebra 1\" width=\"30%\" />\n","    <img src=\"images/zebra2_results.png\" alt=\"Zebra 2\" width=\"30%\" />\n","    <img src=\"images/zebra1_200.png\" alt=\"Zebra 3\" width=\"30%\" />\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"margin-left: 40px;\">\n","\n","#### Quantitative Results:\n","\n","|          | FID    | KID    |\n","|----------|--------|--------|\n","| Train    | 195.77 | 0.135  |\n","| Test     | 120.06 | 0.089  |\n","\n","\n","\n","\n","#### Plotting FID, KID, and losses for training:\n","\n","<div style=\"text-align:center\">\n","    <img src=\"images/FID_train_200.png\" alt=\"FID\" width=\"40%\" />\n","    <img src=\"images/KID_train_200.png\" alt=\"KID\" width=\"40%\" />\n","    <img src=\"images/losses_200.png\" alt=\"losses\" width=\"40%\" />\n","</div>\n","\n","#### Plotting FID and KID for testing:\n","\n","<div style=\"text-align:center\">\n","    <img src=\"images/FID_test_200.png\" alt=\"FID\" width=\"30%\" style=\"margin-right: 20px;\" />\n","    <img src=\"images/KID_test_200.png\" alt=\"KID\" width=\"30%\" style=\"margin-left: 20px;\" />\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["- #### Training dataset with 400 images for trainA and trainB for 80 epochs "]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"text-align:center\">\n","    <img src=\"images/zebra1_400.png\" alt=\"Zebra 1\" width=\"30%\" />\n","    <img src=\"images/zebra2_400.png\" alt=\"Zebra 2\" width=\"30%\" />\n","    <img src=\"images/zebra3_results.jpg\" alt=\"Zebra 3\" width=\"30%\" />\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"margin-left: 40px;\">\n","\n","#### Results on 400 epochs:\n","|          | FID                | KID                |\n","|------------|--------------------|--------------------|\n","| Train | 197.76 | 0.131 |\n","| Test  | 101.70 | 0.082 |\n","\n","#### Plotting FID, KID an losses for training:\n","<div style=\"text-align:center\">\n","    <img src=\"images/FID__train_400.png\" alt=\"FID\" width=\"40%\" />\n","    <img src=\"images/KID_train_400.png\" alt=\"KID\" width=\"40%\" />\n","    <img src=\"images/losses_400.png\" alt=\"losses\" width=\"40%\" />\n","</div>\n","\n","#### Plotting FID and KID for testing:\n","<div style=\"text-align:center\">\n","    <img src=\"images/FID_test_400.png\" alt=\"FID\" width=\"30%\" style=\"margin-right: 20px;\" />\n","    <img src=\"images/KID_test_400.png\" alt=\"KID\" width=\"30%\" style=\"margin-left: 20px;\" />\n","</div>\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5103339,"sourceId":8542175,"sourceType":"datasetVersion"},{"datasetId":5115370,"sourceId":8558693,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
